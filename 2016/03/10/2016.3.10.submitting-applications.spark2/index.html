<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <title>Spark 部署文档-如何向 Spark 上提交应用程序-2 | Kylin&#39;s Blog</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="Descriptions:最近正在系统阅读 Spark 官方文档，阅读的同时也试着翻译了其中部分的章节，本篇文档原文链接地址:http://spark.apache.org/docs/latest/submitting-applications.html#advanced-dependency-management ，个人水平有限,若翻译有不恰当之处欢迎指正，邮箱地址 kylin27@outloo">
<meta property="og:type" content="article">
<meta property="og:title" content="Spark 部署文档-如何向 Spark 上提交应用程序-2">
<meta property="og:url" content="http://kylin27.github.io/2016/03/10/2016.3.10.submitting-applications.spark2/index.html">
<meta property="og:site_name" content="Kylin's Blog">
<meta property="og:description" content="Descriptions:最近正在系统阅读 Spark 官方文档，阅读的同时也试着翻译了其中部分的章节，本篇文档原文链接地址:http://spark.apache.org/docs/latest/submitting-applications.html#advanced-dependency-management ，个人水平有限,若翻译有不恰当之处欢迎指正，邮箱地址 kylin27@outloo">
<meta property="og:updated_time" content="2016-03-11T03:43:50.891Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Spark 部署文档-如何向 Spark 上提交应用程序-2">
<meta name="twitter:description" content="Descriptions:最近正在系统阅读 Spark 官方文档，阅读的同时也试着翻译了其中部分的章节，本篇文档原文链接地址:http://spark.apache.org/docs/latest/submitting-applications.html#advanced-dependency-management ，个人水平有限,若翻译有不恰当之处欢迎指正，邮箱地址 kylin27@outloo">
  
    <link rel="alternative" href="/atom.xml" title="Kylin&#39;s Blog" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  <link rel="stylesheet" href="/css/style.css" type="text/css">
  

</head>
<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Kylin&#39;s Blog</a>
      </h1>
      
        <h2 id="subtitle-wrap">
          <a href="/" id="subtitle">Kylin27@outlook.com</a>
        </h2>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" results="0" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://kylin27.github.io"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main"><article id="post-2016.3.10.submitting-applications.spark2" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2016/03/10/2016.3.10.submitting-applications.spark2/" class="article-date">
  <time datetime="2016-03-09T16:00:00.000Z" itemprop="datePublished">2016-03-10</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      Spark 部署文档-如何向 Spark 上提交应用程序-2
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>Descriptions:<br>最近正在系统阅读 Spark 官方文档，阅读的同时也试着翻译了其中部分的章节，本篇文档原文链接地址:<a href="http://spark.apache.org/docs/latest/submitting-applications.html#advanced-dependency-management" target="_blank" rel="external">http://spark.apache.org/docs/latest/submitting-applications.html#advanced-dependency-management</a> ，<br>个人水平有限,若翻译有不恰当之处欢迎指正，邮箱地址 kylin27@outlook.com </p>
<hr>
<h1 id="Submitting_Applications"><a href="#Submitting_Applications" class="headerlink" title="Submitting Applications"></a>Submitting Applications</h1><h1 id="u5982_u4F55_u5411_Spark__u4E0A_u63D0_u4EA4_u5E94_u7528_u7A0B_u5E8F"><a href="#u5982_u4F55_u5411_Spark__u4E0A_u63D0_u4EA4_u5E94_u7528_u7A0B_u5E8F" class="headerlink" title="如何向 Spark 上提交应用程序"></a>如何向 Spark 上提交应用程序</h1><p>The spark-submit script in Spark’s bin directory is used to launch applications on a cluster.<br>位于 Spark bin 目录下面的 spark-submit 脚本是用来在集群上启动应用程序的。</p>
<p>It can use all Spark’s supported cluster <a href="http://spark.apache.org/docs/latest/cluster-overview.html#cluster-manager-types" target="_blank" rel="external">cluster managers</a> through a uniform interface so you don’t have to configure your application specially for each one.<br>spark-submit 启动脚本通过统一接口来支持 Spark 中的多种集群(资源)管理器(译者: 比如说 Spark 自带的资源管理器，或是 Mesos ，YARN 这种第三方资源调度框架)，所以你无需因为管理器的不同来重新配置你的应用程序。</p>
<h2 id="Building_Your_Applications_u2019s_Dependencies"><a href="#Building_Your_Applications_u2019s_Dependencies" class="headerlink" title="Building Your Applications’s Dependencies"></a>Building Your Applications’s Dependencies</h2><h2 id="u5982_u4F55_u6784_u5EFA_u4F9D_u8D56_u4E8E_u5176_u4ED6_u5E93_u7684_u5E94_u7528_u7A0B_u5E8F"><a href="#u5982_u4F55_u6784_u5EFA_u4F9D_u8D56_u4E8E_u5176_u4ED6_u5E93_u7684_u5E94_u7528_u7A0B_u5E8F" class="headerlink" title="如何构建依赖于其他库的应用程序"></a>如何构建依赖于其他库的应用程序</h2><p>If your code depends on other projects, you will need to package them alongside your application in order to distribute the code to a Spark cluster.<br>如果你所编写的代码依赖于其他的项目的话，建议你在打包时将代码依赖的项目和你所编写的应用放到一起，这样的话可以保证依赖项目可以随代码一起被分发到 Spark 集群上。</p>
<p>To do this, create an assembly jar(or “uber” jar) containing your code and its dependencies.<br>可以通过创建一个组装 Jar 包，在该 Jar 中既包含你编写的代码又包含代码所依赖的文件，这样的话便可以实现自定义代码所依赖项目会随应用程序一起被分发了。</p>
<p>Both <a href="/">sbt</a> and <a href="/">Maven</a> have assembly plugins.<br><a href="/">sbt</a>和 <a href="/">Maven</a> 这两种项目管理工具均有可生成上述的组装 Jar 包的插件。</p>
<p>When creating assembly jars, list Spark and Hadoop as provided dependencies; these need not be bundled since they are provided by the cluster manager at runtime.<br>当打包组装 Jar 包的时候，会将你所编写代码所依赖的 Spark 和 Hadoop 库显示出来； 不过你并不需要将 Spark 和 Hadoop 的库文件加载到 Jar 包中的，因为这些依赖文件将会在应用程序的运行期由 Spark 集群资源管理器来提供。</p>
<p>Once you have an assembled jar you can call the bin/spark-submit script as shown here while passing your jar.<br>一旦 Jar 文件被成功创建，你便可以通过调用位于 bin 目录下面的也是当前我们正在介绍的这个 spark-submit 脚本来将你的 Jar 文件提交到 Spark 集群上了。</p>
<p>For Python, you can use the –py-files argument of spark-submit to add .py, .zip or .egg files to be distributed with your application.<br>如果依赖的是 Python 语言编写的库的话，你可以在使用 spark-submit 脚本的时候通过后接 –py-files 这个参数选项来将以 .py，.zip 或是 .egg 为后缀文件和你的应用一起发布到 Spark 集群上面。</p>
<p>If you depend on multiple Python files we recommend packaging them into a .zip or .egg.<br>如果你的应用程序依赖于不止一个 Python 文件，建议你将多个 Python 文件打包成 .zip 或是 .egg 类型的文件。</p>
<h2 id="Launching_Applications_with_spark-submit"><a href="#Launching_Applications_with_spark-submit" class="headerlink" title="Launching Applications with spark-submit"></a>Launching Applications with spark-submit</h2><h2 id="u4F7F_u7528_spark-submit__u811A_u672C_u6765_u53D1_u5E03_u5E94_u7528_u7A0B_u5E8F_u5230_Spark__u96C6_u7FA4"><a href="#u4F7F_u7528_spark-submit__u811A_u672C_u6765_u53D1_u5E03_u5E94_u7528_u7A0B_u5E8F_u5230_Spark__u96C6_u7FA4" class="headerlink" title="使用 spark-submit 脚本来发布应用程序到 Spark 集群"></a>使用 spark-submit 脚本来发布应用程序到 Spark 集群</h2><p>Once a user application is bundled, it can be launched using the bin/spark-submit script.<br>一旦用户自定义应用程序和依赖文件被成功绑定，便可以使用 bin 路径下的 spark-submit 脚本将其发布到 Spark 上。</p>
<p>This script takes care of setting up the classpath with Spark and its dependencies, and can support different cluster managers and deploy modes that Sparks supports.<br>spark-submit 脚本支持为 Spark 和其所依赖的库来设定 classpath，同时 spark-submit 启动脚本后接的多种参数选项可以很好的配合 Spark 支持的多种集群管理器与启动模式:</p>
<p><b> ps: 由于博客支持符号有限，将下面的 # 替换成左右尖括号</b></p>
<pre>
./bin/spark-submit \
  --class  #main-class#     \
  --master #master-url#      \
  --deploy-mode #deploy-mode# \
  --conf #key#=#value#           \
  ... # other options 
  #application-jar#                \
  [application-arguments]
</pre>

<p>Some of the commonly used options are:<br>spark-submit 脚本中常用的参数选项描述如下:</p>
<ul>
<li>–class: The entry point for your application(e.g. org.apache.spark.examples.SparkPi)</li>
<li><p>–class: 这个参数选项是用来指定整个应用程序的入口点的(例如， Spark 源码包中的 SparkPi 这个类就可以看做是整个应用的入口点)。</p>
</li>
<li><p>–master: The <a href="http://spark.apache.org/docs/latest/submitting-applications.html#master-urls" target="_blank" rel="external">master URL</a> for the cluster(e.g. spark://23.195.26.187:7077)</p>
</li>
<li><p>–master: 这个参数选项是用来指定集群<a href="http://spark.apache.org/docs/latest/submitting-applications.html#master-urls" target="_blank" rel="external">主节点的 URL 地址</a>的(例如你可以将主结点的 IP 设定为 23.195.26.187:7077 然后通过 spark://23.195.26.187:7077 来访问主结点)。</p>
</li>
<li><p>–deploy-mode: Whether to deploy your driver on the worker nodes(cluster) or locally as an external client(client)(default: client)</p>
</li>
<li><p>–deploy-mode: 这个选项是用来设定你在哪里启动驱动程序的，是在工作结点上(这个是集群模式)还是作为外部客户端在本地启动的(这个是客户端模式)，默认的缺省部署模式是本地启动的客户模式。</p>
</li>
<li><p>–conf: Arbitrary Spark configuration property in key=value format. For values that contain spaces wrap “key=value” in quotes(as shown).</p>
</li>
<li><p>–conf: Spark 中用来以键值对方式强制改写配置信息的参数选项。如果键值对的数值中有空格，可以使用引号来包装”键=值”。(像这样 “键=值”)</p>
</li>
<li><p>application-jar: Path to a bundled jar including your application and all dependencies. The URL must be globally visible inside of your cluster, for instance, an hdfs://path of a file:// path that is present on all nodes.</p>
</li>
<li><p>application-jar: 这个参数选项用来指定包含你的应用程序和其所依赖的文件在内绑定好的 jar 文件的路径。 如果使用 URL 来定位的话，这个 URL 对于整个集群来说一定要是全局可见的，例如，若是在 HDFS 上面以 hdfs:// 开头， 或是如果文件路径在所有结点上都存在则可以以 file:// 来开头。  </p>
</li>
<li><p>application-arguments: Arguments passed to the main method of your main class,if any.</p>
</li>
<li>application-arguments: 如果你所编写的应用程序的主函数入口在运行的时候需要传参的话，使用这个参数选项并后接需要传递给主函数的参数，参数便可以正确地传递给应用程序的主函数。</li>
</ul>
<p>A common deployment strategy is to submit your application from a gateway machine that is physically co-located with your worker machines(e.g. Master node in a standalone EC2 cluster).<br>一种比较常用的在 Spark 集群上部署应用程序的方式是在网关主机上提交你的应用，这个网关主机指的就是在集群中和其他工作主机通过物理网络是互相可达的。(例如在独立模式的 EC2 集群中的主结点就扮演着网关主机的角色)。</p>
<p>In this setup, client mode is appropriate. In client mode, the driver is launched directly within the spark-submit process which acts as a client to the cluster.<br>启动 Spark 时， 客户/本地模式是很推荐的启动模式。 在客户模式下启动，驱动程序会以进程成员的方式随 spark-submit 一起直接被启动，该进程扮演着访问集群的客户端的角色。</p>
<p>The input and output of the application is attached to the console.<br>应用程序的输入输出信息可通过控制台上来直接访问。</p>
<p>Thus, this mode is especially suitable for applications that involve the REPL(e.g. Spark shell).<br>正因如此这种模式尤其适合使用到 REPL 表达式(例如 Spark shell 脚本)的应用程序。</p>
<p>Alternatively, if your application is submitted from a machine far from the worker machines(e.g. locally on your laptop), it is common to use cluster mode to minimize network latency between the drivers and the executors.<br>相应的，如果你提交应用程序的主机距离其所支配的工作主机很远的话(比如说，你使用笔记本来执行本地任务提交)，推荐你使用集群这种部署方式来启动 Spark 以减少驱动程序和执行器二者之间网络通信所带来的延迟。</p>
<p>Note that cluster mode is currently not supported for Mesos clusters.<br>值得注意的是，目前 Spark 的集群并不适用于由 Mesos 管理的集群中的。</p>
<p>Currently only YARN supports cluster mode for Python applications.<br>目前只有基于 YARN 管理器的 Spark 集群支持 Python 应用程序的运行。</p>
<p>For Python applications, simply pass a .py file in the place of <application-jar> instead of a JAR, and Python .zip, .egg or .py files to the search path with –py-files.<br>关于如何向 Spark 集群提交 Python 应用程序，只需要简单地将在参数选项 <application-jar> 后面原本追加 JAR 文件的地方使用你所要上传的 .py 文件即可，如果是多个 .py 文件的话，可以将其打包成 .zip ，.egg 文件包或者使用 –py-files 参数来制定该多个 .py 文件的搜索路径名称都可以。</application-jar></application-jar></p>
<p>There are a few options available that are specific to the <a href="http://spark.apache.org/docs/latest/cluster-overview.html#cluster-manager-types" target="_blank" rel="external">cluster manager</a> that is being used.<br>专门用于处于运行中状态的集群管理器的命令参数选项并不多。</p>
<p>For example, with a <a href="http://spark.apache.org/docs/latest/spark-standalone.html" target="_blank" rel="external">Spark standalone cluster</a> with cluster deploy mode, you can also specify –supervise to make sure that the driver is automatically restarted if it fails with non-zero exit code.<br>例如以<a href="/">独立模式启动的 Spark</a>你可以通过 –supervise 这个命令参数是来确保当驱动程序以返回值非零的状态退出之后(译者:也就是错误状态退出的时候)，该驱动程序可以实现自动重启。</p>
<p>To enumerate all such options avaialable to spark-submit, run it with –help. Here are a few examples of common options:<br>如果想一一列举 spark-submit 脚本可用的参数选项信息的话，可以在启动 spark-submit 脚本的时候输入 –help 选项。 下面是使用 spark-submit 脚本常用的参数选项举例:</p>
<pre>
# Run application locally on 8 cores
# 以本地 8 核的方式来向 Spark 提交应用程序

./bin/spark-submit \
  --class org.apache.spark.examples.SparkPi \
  --master local[8]  \
  /path/to/examples.jar \
  100

译者注: 上述的 spark-submit 提交命令解释入下
--class org.apache.spark.examples.SparkPi 
这条命令所对应的是用于指定提交给 Spark 的 Jar 文件的主入口函数的
(也就是包含 main() 入口函数的类，并且在其中成功创建 SparkContext 对象实例的所在类)所在类

--master 
这个参数选项，应该指定(暂时还不太清楚这个)

/path/to/examples.jar 
这个参数选项用来指定提交给 Spark 的 Jar 文件在本地的路径信息

100 
这个参数是用来传递给 Jar 中的主入口函数在启动的时候需要向 
main(String [] args ){...} 传入的参数，
当然如果主入口函数不需要传入参数的话，这个参数选项可以不加。 
</pre>

<pre>
# Run on a Spark standalone cluster in client deploy mode
# 以客户部署模式来启动独立 Spark 集群

./bin/spark-submit \
  --class org.apache.spark.examples.SparkPi \
  --master spark://207.184.161.138:7077 \
  --executor-memory 20G  \
  --total-executor-cores 100 \
  /path/to/examples.jar  \
  1000

译者注: 上述的 spark-submit 提交命令注释如下
--class org.apache.spark.examples.SparkPi
主函数入口所在类

--master spark://207.184.161.138:7077
用 URL 来定位 Spark 集群中的主结点

--executor-memory 20G 
用来为每个执行器进程分配内存空间

--total-executor-cores 100 
用来指定 Spark 开启多少个执行者进程

/path/to/examples.jar 
用来指定用户提交 Jar 文件所在目录信息

1000
用户提交的 Jar 文件中主函数启动所需要的参数

</pre>

<pre>
# Run on a Spark standalone cluster in cluster deploy mode with supervise
# 使用监控着来以集群部署模式来启动独立 Spark 集群

./bin/spark-submit  \
  --class org.apache.spark.examples.SparkPi \
  --master spark://207.184.161.138:7077 \
  --deploy-mode cluster
  --supervise
  --executor-memory 20G \
  --total-executor-cores 100 \
  /path/to/examples.jar \
  1000 

</pre>

<pre>
# Run on a YARN cluster
export HADOOP_CONF_DIR=XXX
./bin/spark-submit \
 --class org.apache.spark.examples.SparkPi \
 --master yarn \
 --deploy-mode cluster \  # can be client for client mod
 --executor-memory 20G \
 --num-executors 50  \
 /path/to/examples.jar \
 1000

</pre>

<pre>
# Run a Python application on a Spark standalone cluster
# 在独立的 Spark 集群上运行 Python 应用程序

./bin/spark-submit   \
  --master spark://207.184.161,138:7077 \
  examples/src/main/python/pi.py  \
  1000

</pre>

<pre>
# Run on a Mesos cluster in cluster deploy mode with supervise 
# 使用 supervise 监控在集群部署模式下启动运行在 Mesos 集群上的 Spark 

./bin/spark-submit \
  --class org.apache.spark.examples.SparkPi \
  --master mesos://207.184.161.138:7077 \
  --deploy-mode cluster
  --supervise
  --executor-memory 20G \
  --total-executor-cores 100 \
  http://path/to/examples.jar \
  1000
</pre>

<h2 id="Master_URLs"><a href="#Master_URLs" class="headerlink" title="Master URLs"></a>Master URLs</h2><h2 id="u5173_u4E8E_u4E3B_u7ED3_u70B9_URL__u5730_u5740_u63CF_u8FF0_u683C_u5F0F"><a href="#u5173_u4E8E_u4E3B_u7ED3_u70B9_URL__u5730_u5740_u63CF_u8FF0_u683C_u5F0F" class="headerlink" title="关于主结点 URL 地址描述格式"></a>关于主结点 URL 地址描述格式</h2><p>The master URL passed to Spark can be in one of the following formats:<br>主节点的 URL 地址以参数选项 –master 的方式传递给 Spark ，不过该主结点的 URL 地址需要遵循如下描述的格式：</p>
<p>Master URL / Meaning<br>主结点 URL / 该 URL 地址所适用的场合</p>
<ul>
<li>local </li>
<li>Runing Spark locally with one worker thread (i.e. no parallelism at all).</li>
<li>此种主结点 URL 地址描述适用于仅开启一个工作者线程的本地 Spark 运行模式(也就是说，运行于此模式下的 Spark 并不支持并行)</li>
</ul>
<hr>
<ul>
<li>local[K]</li>
<li>Run Spark locally with K worker threads (ideally, set this to the number of cores on your machine).</li>
<li>此证 URL 地址描述适用于开启 K 个工作者线程且本地模式启动 Spark 的场景，(通常情况下，理想的方式是运行 Spark 主机 CPU 中有多少个核便相应地开启多少个线程)</li>
</ul>
<hr>
<ul>
<li>local[*]</li>
<li>Run Spark locally with as many worker threads as logical cores on your machine.</li>
<li>本地模式启动 Spark 且让当前主机的 CPU 中的内核 AT 力场全开的运行尽量多的线程。</li>
</ul>
<hr>
<ul>
<li>spark://HOST:PORT</li>
<li>Connect to the given <a href="http://spark.apache.org/docs/latest/spark-standalone.html" target="_blank" rel="external">Spark standalone cluster</a> master. The port must be whichever one your master is configured to use, which is 7077 by default.</li>
<li>这种 URL 地址适用于用于连接以<a href="http://spark.apache.org/docs/latest/spark-standalone.html" target="_blank" rel="external">独立集群的方式启动的 Spark</a> 上的主结点。 端口号要先对其进行配置然后才可以使用，默认的端口号是 7077 .</li>
</ul>
<hr>
<ul>
<li>mesos://HOST:PORT</li>
<li>Connect to the given <a href="http://spark.apache.org/docs/latest/running-on-mesos.html" target="_blank" rel="external">Mesos</a> cluster. The port must be whichever one your is configured to use, which 5050 by default. Or, for a Mesos cluster using Zookeeper, use mesos://zk://…. To submit with –deploy-mode cluster, the HOST:PORT should be configured to connect to the <a href="http://spark.apache.org/docs/latest/running-on-mesos.html#cluster-mode" target="_blank" rel="external">MesosClusterDispatcher</a>.</li>
<li>当想要连接使用 <a href="http://spark.apache.org/docs/latest/running-on-mesos.html" target="_blank" rel="external">Mesos</a> 管理的 Spark 集群中的主结点的时候，使用这个 URL 地址就对了。 端口号在使用之前必须要通过相关配置文件加以设定，缺省端口号是 5050. 或者是， 如果这个 Mesos 使用 Zookeeper 框架的话，那么 URL 地址就相应地变成 mesos://zk://….。 可以使用 –deploy-mode cluster 来向 Mesos 组织的 Spark 集群上提交应用程序，而 HOST:PORT 应该被配置成 <a href="http://spark.apache.org/docs/latest/running-on-mesos.html#cluster-mode" target="_blank" rel="external">Mesos 集群分配器</a>的 URL 访问地址。</li>
</ul>
<hr>
<ul>
<li>yarn</li>
<li>Connect to a <a href="http://spark.apache.org/docs/latest/running-on-yarn.html" target="_blank" rel="external">YARN</a> cluster in client or cluster mode depending on the value of –deploy-mode. The cluster location will be found based on the HADOOP_CONF_DIR or YARN_CONF_DIR variable.</li>
<li>yarn 常用作连接使用 <a href="http://spark.apache.org/docs/latest/running-on-yarn.html" target="_blank" rel="external">YARN</a> 管理器的以本地客户端模式或是集群模式启动的 Spark 集群中的主结点， yarn 是 –deploy-mode 这个参数选项的后接值。 集群的地址可通过查找相关配置文件中的 HADOOP_CONF_DIR 或是 YARN_CONF_DIR 环境变量值来定位。</li>
</ul>
<hr>
<ul>
<li>yarn-client</li>
<li>Equivalent to yarn with –deploy-mode client, which is preferred to ‘yarn-client’.</li>
<li>该 URL 等同于以 yarn 的客户/本地模式启动 Spark 的时候后接 –deploy-mode 这个参数选项，不过前者更加被人所熟知。</li>
</ul>
<hr>
<ul>
<li>yarn-cluster</li>
<li>Equivalent to yarn with –deploy-mode cluster, which is prefered to ‘yarn-cluster’</li>
<li>这个 URL 地址使用与以 yarn 集群 方式启动的时候使用 –deploy-mode cluster 这个参数选项，也通常人们更喜欢用 ‘yarn-cluster’ 。</li>
</ul>
<h2 id="Loading_Configuration_from_a_File"><a href="#Loading_Configuration_from_a_File" class="headerlink" title="Loading Configuration from a File"></a>Loading Configuration from a File</h2><h2 id="u4ECE_u6587_u4EF6_u4E2D_u6765_u52A0_u8F7D_u914D_u7F6E_u9009_u9879"><a href="#u4ECE_u6587_u4EF6_u4E2D_u6765_u52A0_u8F7D_u914D_u7F6E_u9009_u9879" class="headerlink" title="从文件中来加载配置选项"></a>从文件中来加载配置选项</h2><p>The spark-submit script can load default <a href="http://spark.apache.org/docs/latest/configuration.html" target="_blank" rel="external">Spark configuration values</a> from a properties file and pass them on to your application.<br>Spark 中用于提交应用的 spark-submit 脚本可以从配置文件中加载默认的 Spark 配置信息，并将这些配置信息应用在你所提交的应用程序中。</p>
<p>By default it will read options from conf/spark-defaults.conf in the Spark directory.<br>默认情况下， spark-submit 脚本会从 Spark 所在路径下的 conf/spark-default.conf 配置文件中来读取配置选项信息。</p>
<p>For more detail, see the section on <a href="http://spark.apache.org/docs/latest/configuration.html#loading-default-configurations" target="_blank" rel="external">loading default configurations</a>.<br>若想要进一步了解关于配置信息加载的问题，可以查阅<a href="http://spark.apache.org/docs/latest/configuration.html#loading-default-configurations" target="_blank" rel="external">如何加载默认配置信息</a>这篇文章。</p>
<p>Loading default Spark configurations this way can obviate the need for certain flags to spark-submit. For instance, if the spark.master property is set, you can safely omit the –master flag from spark-submit.<br>如果 Spark 配置文件书写得当的话，可以减少在运行 spark-submit 脚本的时候后接参数的数目。 例如，如果 spark.master 这个参数的选项已经在配置脚本中设定好了，那么在调用 spark-submit 的时候可以省去不写 –master 这个参数选项。 </p>
<p>In general, configuration values explicitly set on a SparkConf take the highest precedence, then flags passed to spark-submit, then values in the defaults file.<br>通常情况下不同方式的配置数值之间是有着明确的优先级的，其中通过 SparkConf 对象实例所设定的配置参数享有最高的优先级， 接下来是运行 spark-submit 脚本时所传递的参数，最后是写在配置文件中的默认选项信息。</p>
<p>If your are ever unclear where configuration options are coming from, you can print out fine-grained debugging information by running spark-submit with the –verose option.<br>如果你对 Spark 中的某些配置选项不是很理解，不知道某些参数是用来做什么的话，建议在运行 spark-submit 脚本的时后接 –verose 参数这个选项，这样就可以将每个后接参数的调试信息详细打印出来了。</p>
<h2 id="Advanced_Dependency_Management"><a href="#Advanced_Dependency_Management" class="headerlink" title="Advanced Dependency Management"></a>Advanced Dependency Management</h2><p>##<br>When using spark-submit, the application jar along with any jars included with the –jars option will be automatically transferred to the cluster.<br>当运行 spark-submit 脚本时，应用程序连同任何 –jars 参数选项后接的 jar 文件都会被自动的提交给集群。</p>
<p>Spark uses the following URL scheme to allow different strategies for disseminating jars:<br>Spark 使用如下所示的多种 URL 模式来实现不同策略的 jar 文件的分发:</p>
<ul>
<li>file: - Absolute path and file:/URIs are servered by the driver’s HTTP file server, and every executor pulls the file from the driver HTTP server.</li>
<li><p>file: - 此种文件描述方式使用的是本地文件所在的绝对路径地址以及使用 file:/URL 的文件路径描述方式是由文件 HTTP 服务器所提供的文件定位服务。位于每个结点上的执行器也均是从 HTTP 文件服务器上来抽取其所需要的文件。 </p>
</li>
<li><p>hdfs:, http:, https:, ftp: - these pull down files and JARs from the URI as expected</p>
</li>
<li>hdfs： 此种文件路径描述是以 http, https, ftp 文件传输协议根据文件的 URI 描述地址来抓取普通和 JAR 类型的文件的。 </li>
</ul>
<ul>
<li>local: - a URI starting with local:/ is expected to exist as a local file on each worker node. This means that no network IO will be incurred, and works well for large files/JARs that are pushed to each worker, or shared via NFS, ClusterFS, etc. </li>
<li>使用 local 来作为文件路径描述的话，通常是按照 local:/ + 文件的 URI 地址这种格式，不过若以次种文件描述方式，需要每个工作结点的本地路径上都需要有该文件的备份。 这便意味着在整个过程中不会涉及到文件在网络进行传输，且由于大块的普通 JAR 文件都已经被推送到每个工作结点本地或是以 NFS, ClusterFS 文件系统的方式进行共享，所以在这种情景下其工作效率十分的高效。<br>(译者注: 不过将大文件冗余地存放到如此多的结点上所带来的开销也是很大的，类似于算法中的空间与时间二者之间的权衡)</li>
</ul>
<p>Note that JARs and files are copied to the working directory for each SparkContext on the executor nodes.<br>需要知道的是 JAR 和 普通文件会被拷贝到位于每个执行结点上所创建的的 SparkContext 的工作路径下。</p>
<p>This can use up a significant amount of space over time and will need to be cleaned up.<br>这是一种会随着时间推移十分吃内存的处理方法，所以会涉及到空间的清理与回收操作。</p>
<p>With YARN, cleanup is handled automatically, and with Spark standalone, automatic cleanup can be configured with the spark.worker.cleanup.appDataTtl property.<br>如果使用 YARN 这种资源管理框架的话，空间资源的清理与回收会被自动的执行；如果使用 Spark 自带的资源管理器的话，可以通过配置 spark.worker.cleanup.appDataTlt 这个选项来决定让 Spark 是否能够自动清理与回收空间。</p>
<p>Users may also included any other dependencies by supplying a comma-delimited list of maven coordinates with –packages.<br>若是用户希望包含其他 maven 相关的依赖文件，还在命令后配合使用 –packages 这个参数选项；若是不止一个依赖文件可使用逗号作为分隔符。</p>
<p>All transitive dependencies will be handled when using this command.<br>使用上述的这个参数选项的话，依赖文件的所有隐式依赖文件也一并会被处理。</p>
<p>Additional repositories( or resolvers in SBT) can be added in a comma-delimited fashion with the flag –repositories.<br>另外，项目代码资源库(或是使用 SBT 作为版本控制项目)也可以以逗号分隔符的方式配合 –repositories 这个参数选项来使用。</p>
<p>These commands can be used with pyspark, spark-shell, and spark-submit to include Spark packages.<br>上述的这些命令可以配合 Spark 安装包中的 pyspark,spark-shell 和 spark-submit 这些脚本中的任意一个脚本使用。</p>
<p>For Python, the equivalent –py-files option can be used to distribute .egg, .zip and .py libraries to executors.<br>如果是上传以 Python 编写的项目代码的话，可以相应地使用 –py-files 这个参数选项来将打包成 .egg,.zip 的 python 文件，或是 python 库文件本身分发到位于每个结点的执行器上。</p>
<h2 id="More_Information"><a href="#More_Information" class="headerlink" title="More Information"></a>More Information</h2><h2 id="u5173_u4E8E_u8FDB_u4E00_u6B65_u5B66_u4E60"><a href="#u5173_u4E8E_u8FDB_u4E00_u6B65_u5B66_u4E60" class="headerlink" title="关于进一步学习"></a>关于进一步学习</h2><p>Once you have deployed your application, the <a href="http://spark.apache.org/docs/latest/cluster-overview.html" target="_blank" rel="external">cluster mode overview</a> describes the components involved in distributed execution, and how to monitor and debug applications.<br>一旦你将你的应用部署到集群之上，在<a href="http://spark.apache.org/docs/latest/cluster-overview.html" target="_blank" rel="external">关于集群模式概览</a>一文中介绍了 Spark 在分布式环境下运行时所需要的组件，以及如何监控与调试你所部署的应用程序。</p>
<p>end  </p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://kylin27.github.io/2016/03/10/2016.3.10.submitting-applications.spark2/" data-id="ciln5qws2000txkimhniq4kt1" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Spark/">Spark</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/official-doc/">official-doc</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/翻译/">翻译</a></li></ul>

    </footer>
  </div>
  
    
<nav id="article-nav">
  
  
    <a href="/2016/03/09/2016.3.8.cluster-overview1/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">Spark 部署文档-Spark 集群模式概览-1</div>
    </a>
  
</nav>

  
</article>

</section>
        
          <aside id="sidebar">
  
    
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tags</h3>
    <div class="widget">
      <ul class="tag-list"><li class="tag-list-item"><a class="tag-list-link" href="/tags/C/">C++</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Chord/">Chord</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/DHT/">DHT</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Gtest/">Gtest</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/IDEA/">IDEA</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Intellij/">Intellij</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Linux/">Linux</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Node-js/">Node.js</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/P2P/">P2P</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Spark/">Spark</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/StackExchangeRecommenderSystem-repo/">StackExchangeRecommenderSystem_repo</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Ubuntu/">Ubuntu</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/algorithm/">algorithm</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/blog/">blog</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/cluster/">cluster</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/concurrency/">concurrency</a><span class="tag-list-count">3</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/datastructure/">datastructure</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/debug/">debug</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/doc/">doc</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/docker/">docker</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/dockerfile/">dockerfile</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/github/">github</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/gtest/">gtest</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/hexo/">hexo</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/java/">java</a><span class="tag-list-count">5</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/log4j/">log4j</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/manual/">manual</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/note/">note</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/official/">official</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/official-doc/">official-doc</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/official-guides/">official-guides</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/picture/">picture</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/programming-theory/">programming-theory</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/scala/">scala</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/search-engine/">search-engine</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/spark/">spark</a><span class="tag-list-count">3</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/theory/">theory</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/translate/">translate</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/translation/">translation</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/tutorial/">tutorial</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/windows/">windows</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/七牛/">七牛</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/个人简历/">个人简历</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/测试/">测试</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/翻译/">翻译</a><span class="tag-list-count">3</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/tags/C/" style="font-size: 10px;">C++</a> <a href="/tags/Chord/" style="font-size: 10px;">Chord</a> <a href="/tags/DHT/" style="font-size: 10px;">DHT</a> <a href="/tags/Gtest/" style="font-size: 10px;">Gtest</a> <a href="/tags/IDEA/" style="font-size: 10px;">IDEA</a> <a href="/tags/Intellij/" style="font-size: 10px;">Intellij</a> <a href="/tags/Linux/" style="font-size: 10px;">Linux</a> <a href="/tags/Node-js/" style="font-size: 10px;">Node.js</a> <a href="/tags/P2P/" style="font-size: 10px;">P2P</a> <a href="/tags/Spark/" style="font-size: 13.33px;">Spark</a> <a href="/tags/StackExchangeRecommenderSystem-repo/" style="font-size: 10px;">StackExchangeRecommenderSystem_repo</a> <a href="/tags/Ubuntu/" style="font-size: 10px;">Ubuntu</a> <a href="/tags/algorithm/" style="font-size: 10px;">algorithm</a> <a href="/tags/blog/" style="font-size: 13.33px;">blog</a> <a href="/tags/cluster/" style="font-size: 10px;">cluster</a> <a href="/tags/concurrency/" style="font-size: 16.67px;">concurrency</a> <a href="/tags/datastructure/" style="font-size: 10px;">datastructure</a> <a href="/tags/debug/" style="font-size: 10px;">debug</a> <a href="/tags/doc/" style="font-size: 10px;">doc</a> <a href="/tags/docker/" style="font-size: 13.33px;">docker</a> <a href="/tags/dockerfile/" style="font-size: 10px;">dockerfile</a> <a href="/tags/github/" style="font-size: 13.33px;">github</a> <a href="/tags/gtest/" style="font-size: 10px;">gtest</a> <a href="/tags/hexo/" style="font-size: 13.33px;">hexo</a> <a href="/tags/java/" style="font-size: 20px;">java</a> <a href="/tags/log4j/" style="font-size: 10px;">log4j</a> <a href="/tags/manual/" style="font-size: 10px;">manual</a> <a href="/tags/note/" style="font-size: 10px;">note</a> <a href="/tags/official/" style="font-size: 10px;">official</a> <a href="/tags/official-doc/" style="font-size: 10px;">official-doc</a> <a href="/tags/official-guides/" style="font-size: 10px;">official-guides</a> <a href="/tags/picture/" style="font-size: 10px;">picture</a> <a href="/tags/programming-theory/" style="font-size: 10px;">programming-theory</a> <a href="/tags/scala/" style="font-size: 13.33px;">scala</a> <a href="/tags/search-engine/" style="font-size: 10px;">search-engine</a> <a href="/tags/spark/" style="font-size: 16.67px;">spark</a> <a href="/tags/theory/" style="font-size: 13.33px;">theory</a> <a href="/tags/translate/" style="font-size: 10px;">translate</a> <a href="/tags/translation/" style="font-size: 13.33px;">translation</a> <a href="/tags/tutorial/" style="font-size: 10px;">tutorial</a> <a href="/tags/windows/" style="font-size: 10px;">windows</a> <a href="/tags/七牛/" style="font-size: 10px;">七牛</a> <a href="/tags/个人简历/" style="font-size: 10px;">个人简历</a> <a href="/tags/测试/" style="font-size: 10px;">测试</a> <a href="/tags/翻译/" style="font-size: 16.67px;">翻译</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/03/">March 2016</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/02/">February 2016</a><span class="archive-list-count">15</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/10/">October 2015</a><span class="archive-list-count">1</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recents</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2016/03/10/2016.3.10.submitting-applications.spark2/">Spark 部署文档-如何向 Spark 上提交应用程序-2</a>
          </li>
        
          <li>
            <a href="/2016/03/09/2016.3.8.cluster-overview1/">Spark 部署文档-Spark 集群模式概览-1</a>
          </li>
        
          <li>
            <a href="/2016/02/27/2016.27.java_concurrency.Multithreading_benifits2/">Multithreading Benefits [多线程带来的福利]</a>
          </li>
        
          <li>
            <a href="/2016/02/27/2016.2.27.java_multithreading_costs3/">Multithreading Costs [多线程所花费的代价]</a>
          </li>
        
          <li>
            <a href="/2016/02/27/2016.2.27.java_concurrency.Multithreading Tutorial_1/">Java Concurrency / Multithreading Tutorial [Java 并发/多线程教程]</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2016 Kylin<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css" type="text/css">
  <script src="/fancybox/jquery.fancybox.pack.js" type="text/javascript"></script>


<script src="/js/script.js" type="text/javascript"></script>

  </div>
</body>
</html>