<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <title>Kylin&#39;s Blog</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="Write.Rewrite.When not writing or rewriting, read.I know of no shortcuts.">
<meta property="og:type" content="website">
<meta property="og:title" content="Kylin's Blog">
<meta property="og:url" content="http://kylin27.github.io/index.html">
<meta property="og:site_name" content="Kylin's Blog">
<meta property="og:description" content="Write.Rewrite.When not writing or rewriting, read.I know of no shortcuts.">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Kylin's Blog">
<meta name="twitter:description" content="Write.Rewrite.When not writing or rewriting, read.I know of no shortcuts.">
  
    <link rel="alternative" href="/atom.xml" title="Kylin&#39;s Blog" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  <link rel="stylesheet" href="/css/style.css" type="text/css">
  

</head>
<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Kylin&#39;s Blog</a>
      </h1>
      
        <h2 id="subtitle-wrap">
          <a href="/" id="subtitle">Kylin27@outlook.com</a>
        </h2>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" results="0" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://kylin27.github.io"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main">
  
    <article id="post-2016.3.8.cluster-overview1" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2016/03/09/2016.3.8.cluster-overview1/" class="article-date">
  <time datetime="2016-03-08T16:00:00.000Z" itemprop="datePublished">2016-03-09</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2016/03/09/2016.3.8.cluster-overview1/">Spark 部署文档-Spark 集群模式概览-1</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>Descriptions: 最近正在系统学习 Spark ，本篇文章翻译自 <a href="http://spark.apache.org/docs/latest/cluster-overview.html" target="_blank" rel="external">Spark 官方指导文档</a> 如有不当之处敬请指正，联系邮箱: kylin27@outlook.com</p>
<hr>
<h1 id="Cluster_Mode_Overview"><a href="#Cluster_Mode_Overview" class="headerlink" title="Cluster Mode Overview"></a>Cluster Mode Overview</h1><h1 id="Spark_u96C6_u7FA4_u6A21_u5F0F_u6982_u89C8"><a href="#Spark_u96C6_u7FA4_u6A21_u5F0F_u6982_u89C8" class="headerlink" title="Spark集群模式概览"></a>Spark集群模式概览</h1><p>This document gives a short overview of how Spark runs on clusters, to make it easier to understand the components involved.<br><br>本文档简要介绍了 Spark 的集群运行模式，目的是为了让用户更加容易了解 Spark 中涉及到的组件。</p>
<p>Read through the <a href="http://spark.apache.org/docs/latest/submitting-applications.html" target="_blank" rel="external">application submission guide</a> to learn about launching applications on a cluster.<br><br>用户可以通读<a href="http://spark.apache.org/docs/latest/submitting-applications.html" target="_blank" rel="external">如何向 spark 集群提交应用程序指导手册</a>来了解如何在集群上运行应用程序。</p>
<h2 id="Components"><a href="#Components" class="headerlink" title="Components"></a>Components</h2><h2 id="Spark__u4E2D_u7684_u7EC4_u4EF6"><a href="#Spark__u4E2D_u7684_u7EC4_u4EF6" class="headerlink" title="Spark 中的组件"></a>Spark 中的组件</h2><p>Spark applications run as independent sets of processes on a cluster, coordinated by the SparkContext object in your main program(called the driver program).<br><br>Spark 应用程序以独立的进程集合运行与集群之上，并被位于主程序(这个主程序也被称作是驱动程序)中称作 Spark 上下文(SparkContext)的对象实例所协调调度。</p>
<p>Specifically, to run on a cluster, the SparkContext can connect to several types of cluster managers(either Spark’s own standalone cluster manager, Mesos or YARN), which allocate resources across applications.<br><br>特别值得注意的是，以集群运行的 Spark 中的 SparkContext 可以连接多种类型的用于跨应用程序分配资源的集群调度器(这个调度器既可以是 Spark 自己独立的集群资源管理器，也可以是 Mesos 或是 YARN).</p>
<p>Once connected, Spark acquires executors on nodes in the cluster, which are processes that run computations and store data for your application.<br><br>一旦 Spark 与资源调度器二者相连接，Spark 便会向位于集群中的多个结点索取执行器资源。这些执行器由多个进程构成的专门用来为你的应用程序提供计算和存放数据的功能。</p>
<p>Next, it sends your application code (defined by JAR or Python files passed to SparkContext) to the executors.<br><br>接下来你所提交的应用程序代码(被传递给 Spark 上下文对象实例的应用程序代码是以 JAR 或者是 Python 文件所存放的)会被发送给位于集群中各个结点的执行器上。</p>
<p>Finally, SparkContext sends tasks to the exeuctors to run.<br><br>最后，Spark 上下文对象便会将任务分发给执行器来运行。</p>
<p><img src="http://7xqz39.com1.z0.glb.clouddn.com/hehe_1_%E9%9B%86%E7%BE%A4%E6%A1%86%E6%9E%B6%E6%88%AA%E5%9B%BE.png" alt=""></p>
<p>There are several useful things to note about this architecture:<br>对于上述的这种架构下面是几点有用的建议:</p>
<p><br>1. Each application gets its own executor processes, which stay up for the duration of the whole application and runs tasks in multiple threads. This has the benefit of isolating applications from each other, on both the scheduling side( each driver schedules its own tasks) and executor side (tasks from different applications run in different JVMs). However, it also means that data cannot be shared across different Spark applications (instances of SparkContext) without writing it to an external storage system.<br><br>1. 每个应用程序都有着属于它自己的多个执行器进程，这些执行器进程会在该应用程序的整个生命周期中存活，并且将分配给它们的任务以多线程的方式来运行。 这种机制有利于让应用程序彼此之间保持隔离，无论是资源调度方面的隔离(程序各自的驱动程序调度各自的任务)，还是在程序在执行期间的隔离(隶属于不同应用的任务各自运行在不同的 JVM 中)。然而这种隔离同样也意味着位于 多个Spark 应用程序(就是包含 SparkContext 对象实例的程序)中的数据如果不将其写入到外存存储系统的话是无法在多个 Spark 应用程序间被共享的。<br></p>

<p><br>2. Spark is a agnostic to the underlying cluster manager. As long as it can acquire executor processes, and these communicate with each other, it is relatively easy to run it even on a cluster manager that also supports other applications (e.g. Mesos/YARN).<br><br>2. 在 Spark 集群底层可以使用多种类型的集群调度器来管理(agnostic  不可知，在这里指的是未来的 Spark 集群底层可能不仅仅支持一种资源调度器)。 只要这种资源调度器可以获取 Spark 中的执行进程并且支持进程彼此之间的正常通行就可以，即便将用于管理其他类型集群的像是 Mesos 或是 YARN 这些管理器用于管理 Spark 集群也并不是什么难事。<br></p>

<p><br>3. The driver program must listen for and accept incoming connections from its executors throughout its lifetime(e.g, see <a href="http://spark.apache.org/docs/latest/configuration.html#networking" target="_blank" rel="external">spark.driver.port and spark.fileserver.port in the network config section)</a>. As such, the driver program must be network addressable from the worker nodes.<br><br>3. 驱动程序必须在其整个生命周期内来监听与接收来自其执行器的连接请求(请看在网路配置章节中的 <a href="http://spark.apache.org/docs/latest/configuration.html#networking" target="_blank" rel="external">spark 驱动端口和 spark 文件服务器端口示例文档</a>)。 如此一来，该驱动程序对工作结点来说必须要通过网络地址来定位查找到的才可以。<br></p>

<p><br>4. Because the driver schedules tasks on the cluster, it should be run close to the worker nodes, preferably on the same local area network. If you’d like to send requests to the cluster remotely, its’s better to open an RPC to the driver and have it submit operations from nearby than to run a driver far away from the worker nodes.<br><br>4. 由于驱动程序负责调度集群中的任务， 所以对于驱动程序来说它里其所调度的工作结点物理位置越近越好，最好通过本地地址可以直接访问到其所控制的工作节点。 如果你打算向远程集群发送请求信息的话，最好的处理方式便是通过创建一个到驱动程序 RPC 连接，通过 RPC 来决定向哪一个驱动程序提交请求操作信息，而不是调用一个物理距离其工作结点很远的驱动程序。<br></p>

<h2 id="Cluster_Manager_Types"><a href="#Cluster_Manager_Types" class="headerlink" title="Cluster Manager Types"></a>Cluster Manager Types</h2><p>The system currently supports three cluster managers:</p>
<ul>
<li><a href="http://spark.apache.org/docs/latest/spark-standalone.html" target="_blank" rel="external">Standalone</a> - a simple cluster manager included with Spark that makes it easy to set up a cluster.<br></li>
<li><p>单机模式 - 该模式是基于 Spark 自身包含的集群资源管理器来启动 Spark 的，该资源管理器的作用便是是为了让集群易于启动。</p>
</li>
<li><p><a href="http://spark.apache.org/docs/latest/running-on-mesos.html" target="_blank" rel="external">Apache Mesos</a> - a general cluster manager that can also run Hadoop MapReduce and service applications.<br></p>
</li>
<li>基于 Apache Mesos 的集群模式 - Apache Mesos 是一种普遍被用于集群资源管理的调度器，也可以用作 Hadoop MapReduce 和其他服务应用的资源调度器。</li>
</ul>
<ul>
<li><a href="http://spark.apache.org/docs/latest/running-on-yarn.html" target="_blank" rel="external">Hadoop YARN</a> - the resource manager in Hadoop 2.<br></li>
<li>基于 Hadoop YARN 的集群模式 - Hadoop YARN 是应用于 Hadoop 2版本中的资源管理调度器。</li>
</ul>
<p>In addition, Spark’s <a href="http://spark.apache.org/docs/latest/ec2-scripts.html" target="_blank" rel="external">EC2 launch scripts</a> make it easy to launch a standalone cluster on Amazon EC2.<br><br>除了上述的资源调度器之外， 通过运行 Spark 中的 EC2 启动脚本也可以很容易地在 Amazon 的 EC2 服务器上以单机模式来启动 Spark 。</p>
<h2 id="Submitting_Applications"><a href="#Submitting_Applications" class="headerlink" title="Submitting Applications"></a>Submitting Applications</h2><h2 id="u5173_u4E8E_u5982_u4F55_u5411_Spark__u4E0A_u63D0_u4EA4_u5E94_u7528"><a href="#u5173_u4E8E_u5982_u4F55_u5411_Spark__u4E0A_u63D0_u4EA4_u5E94_u7528" class="headerlink" title="关于如何向 Spark 上提交应用"></a>关于如何向 Spark 上提交应用</h2><p>Applications can be submitted to a cluster of any type using the spark-submit script.<br><br>可以使用Spark 自带的 spark-submit 脚本来将应用程序提交到上述 Spark 的任意一类集群的上。</p>
<p>The <a href="http://spark.apache.org/docs/latest/submitting-applications.html" target="_blank" rel="external">application submission guide</a> describes how to do this.<br><br>这篇 [如何向 Spark 集群提交应用程序教程] 上详细记录了如何提交应用到 Spark 集群。</p>
<h2 id="Monitoring"><a href="#Monitoring" class="headerlink" title="Monitoring"></a>Monitoring</h2><h2 id="u5173_u4E8E_Spark__u96C6_u7FA4_u76D1_u63A7"><a href="#u5173_u4E8E_Spark__u96C6_u7FA4_u76D1_u63A7" class="headerlink" title="关于 Spark 集群监控"></a>关于 Spark 集群监控</h2><p>Each driver program has a web UI, typically on port 4040, that displays information about running tasks, executors, and storage usage. <br><br>每个驱动程序都有其用于展示任务运行，执行器状态以及存储详细信息的 web 图形展示界面，该图形界面的访问端口号通常是 4040。</p>
<p>Simply go to http://<driver-node>:4040 in a web browser to access this UI. The <a href="http://spark.apache.org/docs/latest/monitoring.html" target="_blank" rel="external">monitoring guide</a> also describes other monitoring options.<br><br>(你只要)简单地在浏览器中输入这个网址 http://<driver-node>:4040 Spark 的图形用户界面就展示在你眼前了。</driver-node></driver-node></p>
<p>The <a href="http://spark.apache.org/docs/latest/monitoring.html" target="_blank" rel="external">monitoring guide</a> also describes other monitoring options.<br><br><a href="/">Spark 集群监控教程</a>这一文档中同样也介绍了其他用于 Spark 集群监控的选项。</p>
<h2 id="Job_Scheduling"><a href="#Job_Scheduling" class="headerlink" title="Job Scheduling"></a>Job Scheduling</h2><h2 id="u4F5C_u4E1A_u8C03_u5EA6"><a href="#u4F5C_u4E1A_u8C03_u5EA6" class="headerlink" title="作业调度"></a>作业调度</h2><p>Spark gives control over resource allocation both across applications(at the level of the cluster manager) and within applications(if multiple computations are happening on the same SparkContext). <br><br>Spark 同时在跨应用程序和在应用程序中充当着掌控资源分配的角色，在跨程序资源分配中 Spark 是作为集群管理者来进行资源分配，而当 SparkContext 中同时执行多个计算时 Spark 会在应用程序内进行资源分配。</p>
<p>The <a href="http://spark.apache.org/docs/latest/job-scheduling.html" target="_blank" rel="external">job scheduling overview</a> describes this in more detail.<br><br>在<a href="http://spark.apache.org/docs/latest/job-scheduling.html" target="_blank" rel="external">作业调度概览</a>一文中对此进行了详细的介绍。</p>
<h2 id="Glossary"><a href="#Glossary" class="headerlink" title="Glossary"></a>Glossary</h2><h2 id="Spark__u96C6_u7FA4_u672F_u8BED_u5217_u8868"><a href="#Spark__u96C6_u7FA4_u672F_u8BED_u5217_u8868" class="headerlink" title="Spark 集群术语列表"></a>Spark 集群术语列表</h2><p>The following table summarizes terms you’ll see used to refer to cluster concepts:<br><br>下面表格总结了 Spark 集群中常用到的概念术语:</p>
<p>术语/解释</p>
<ul>
<li><b>Application</b></li>
<li>应用程序</li>
<li>Using program built on Spark. Consists of a driver program and executors on the cluster.<br></li>
<li>运行在 Spark 平台上的程序。(这种程序通常是)由集群上的一个驱动(程序)和多个执行器(程序)所组成的。</li>
</ul>
<hr>
<ul>
<li><b>Application jar</b></li>
<li>jar 文件格式的应用程序</li>
<li>A jar containing the user’s Spark application. In some cases users will want to create an “uber jar” containing their application along with its dependencies. The user’s jar should never include Hadoop or Spark libraries, however, these will be added at runtime.</li>
<li>所谓的 jar 应用程序指的是将用户自己编写的 Spark 代码打包成 jar 应用程序。在某些场合用户会将应用程序和其所依赖的文件打包成一个 “uber jar” 类型文件。 但是值得注意的是，用户是万万不可在生成应用 jar 文件时将应用程序中所依赖的 Hadoop 或是Spark 库文件一并加载到其中的。应用程序中所使用的Hadoop 和 Spark 库会在应用程序的运行期由 Spark 集群平台来提供。</li>
</ul>
<hr>
<ul>
<li><b> Driver program </b></li>
<li>驱动程序</li>
<li>The process running the main() function of the application and creating the SparkContext</li>
<li>驱动程序指的是代码中包含程序入口函数 main() 方法同时在代码中创建 SparkContext 对象的程序。</li>
</ul>
<hr>
<ul>
<li><b>Cluster manager</b></li>
<li>集群调度器/其实叫做集群管理器也行啦~</li>
<li>An external service for acquiring resources on the cluster( e.g. standalone manager, Mesos, YARN)</li>
<li>用于在集群中获取资源的(不属于集群的)外部服务程序(就像是 Spark 单机结点启动的资源管理器，或是第三方像 Mesos ， YARN 这样的资源调度框架)</li>
</ul>
<hr>
<ul>
<li><b>Deploy mode</b></li>
<li>(Spark的)部署模式</li>
<li>Distinguishes where the driver process runs. In “cluster” mode, the framework launches the driver inside of the cluster. In “client” mode, the submitter lanuches the driver of the cluster.</li>
<li>集群的部署模式这一术语是用来区分驱动程序在何处运行的。如果是以”集群”模式运行的，指的是 Spark 这个计算框架在<b>集群中来</b>调用驱动程序的。 如果是以”客户端”模式运行，那便是 Spark 中的 submitter/任务提交者在<b>集群之外</b>调用驱动程序来启动的。</li>
</ul>
<hr>
<ul>
<li><b>Worker node </b></li>
<li>工作结点</li>
<li>Any node that can run application code in the cluster.</li>
<li>在集群中只要能跑应用程序代码的结点都叫做工作结点</li>
</ul>
<hr>
<ul>
<li><b>Executor </b></li>
<li>执行器</li>
<li>A process launched for an application on a worker node, that runs tasks and keeps data in memory or disk storage across them. Each application has its own executors.</li>
<li>在用于运行任务并在内存或是磁盘上保存数据的工作结点上为执行应用程序而启动进程便可以称它是执行器。每个应用都有其自己的一组执行器。</li>
</ul>
<hr>
<ul>
<li><b>Task</b></li>
<li>任务</li>
<li>A unit of work that will be sent to one executor.</li>
<li>任务指的是将会被发送给一个执行器的一系列工作。</li>
</ul>
<hr>
<ul>
<li><b>Job</b></li>
<li>作业</li>
<li>A parallel computation consisting of multiple tasks that gets spawned in response to a Spark action(e.g. save, collect); you’ll see this term used in the driver’s logs.</li>
<li>作业指的是由多个任务所构成的并行计算操作。 这些任务从 Spark 所响应的操作(例如存储，收集数据)中获取执行结果；作业在驱动程序所打印的日志文件中是比较常见的术语。</li>
</ul>
<hr>
<ul>
<li><b>Stage</b></li>
<li>阶段</li>
<li>Each job gets divided into smaller sets of tasks called stages that depend on each other(similar to the map and reduce stages in MapReduce); you’ll see this term used in the driver’s logs.</li>
<li>每份作业在被执行的时候都会被分割成由更小粒度的任务构成的集合。而该任务又被称作是’阶段’，而这些多个阶段彼此间是相互依赖的(就和 MapReduce 中的 map阶段和reduce阶段类似)；’阶段’这一术语在驱动程序打印的日志中比较常见。</li>
</ul>
<p>end</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://kylin27.github.io/2016/03/09/2016.3.8.cluster-overview1/" data-id="cill04baj000mrkimftij42zh" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Spark/">Spark</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/official-guides/">official-guides</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/翻译/">翻译</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-2016.2.27.java_multithreading_costs3" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2016/02/27/2016.2.27.java_multithreading_costs3/" class="article-date">
  <time datetime="2016-02-26T16:00:00.000Z" itemprop="datePublished">2016-02-27</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2016/02/27/2016.2.27.java_multithreading_costs3/">Multithreading Costs [多线程所花费的代价]</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>Descriptions:<br>java 并发文档翻译，文章链接地址 <a href="http://tutorials.jenkov.com/java-concurrency/costs.html" target="_blank" rel="external">http://tutorials.jenkov.com/java-concurrency/costs.html</a></p>
<hr>
<p>Going from a singlethreaded to a multithreaded application doesn’t just provide benifits.<br>将单线程程序修改成多线程好处是有的，但同时也会带来其他方面的损失。</p>
<p>It also has some costs. Don’t just multithread-enable an application just because you can.<br>使用多线程需要为之付出相应的代价。 不要仅仅因为你会多线程便去在编程中使用这种技术。</p>
<p>You should have a good idea that the benefits gained by doing so, are larger than the cost.<br>在使用之前你应该明确的知道相比于应用多线程所花费的代价，你能够获得更大的收益(的时候再来决定将多线程加入到你的程序中)。</p>
<p>When in doubt, try measuring the performance or responsiveness of the application, instead of just guessing.<br>如果你不能很好的做出决断，那么试着权衡一下程序的性能或是响应能力在使用多线程技术前后有何变化，而不是仅仅通过猜测的方式来决定是否使用(多线程)。</p>
<h2 id="More_complex_design"><a href="#More_complex_design" class="headerlink" title="More complex design"></a>More complex design</h2><h2 id="u8BBE_u8BA1_u66F4_u52A0_u590D_u6742"><a href="#u8BBE_u8BA1_u66F4_u52A0_u590D_u6742" class="headerlink" title="设计更加复杂"></a>设计更加复杂</h2><p>Though some parts of a multithreaded applications is simpler than a singlethreaded application, other parts are more complex.<br>虽然多线程应用程序在某些地方的实现要比单线程应用程序要简单的多，但是其他地方的设计要远比单线程设计复杂得多。</p>
<p>Code executed by multiple threads accessing shared data need special attention.<br>被多线程执行的访问共享数据的代码端需要给予高度重视。</p>
<p>Thread interaction is far from always simple.<br>线程之间的交互要远比想象的复杂。</p>
<p>Errors arising from incorrect thread synchronization can be very hard to detect, reproduce and fix.<br>线程同步操作所引发的错误很难被探知，重现或是修复。</p>
<h2 id="Context_Switching_Overhead"><a href="#Context_Switching_Overhead" class="headerlink" title="Context Switching Overhead"></a>Context Switching Overhead</h2><h2 id="u8FC7_u6E21_u4E0A_u4E0B_u6587_u5207_u6362"><a href="#u8FC7_u6E21_u4E0A_u4E0B_u6587_u5207_u6362" class="headerlink" title="过渡上下文切换"></a>过渡上下文切换</h2><p>When a CPU switches from executing one thread to executing another, the CPU needs to save the local data, program pointer etc. of the current thread, and load the local data, program pointer etc. of the next thread to execute.<br>当 CPU 从一个正在执行的线程切换去执行另一个线程的时候，这个 CPU 需要存放当前线程的本地数据,程序指针等信息; 然后加载另一个将要执行的线程中的本地数据，程序指针等信息。</p>
<p>This switch is called a “context switch”.<br>上面所描述的切换就叫做 “上下文切换”。</p>
<p>The CPU switches from executing in the context of one thread to executing in the context of another.<br>对于 CPU 来说，它从正在运行线程的上下文中切换到另一个即将运行的线程的上下文中。</p>
<p>Context switching isn’t cheap.<br>上下文切换的花费高昂。</p>
<p>You don’t want to switch between threads more than necessary.<br>所以不到万不得已，我认为你是不会在两个线程之间进行上下文切换的。</p>
<p>You can read more about context switching on Wikipedia:<br>你可以通过查阅维基百科来获取更多关于上下文切换相关的知识:</p>
<p><a href="http://en.wikipedia.org/wiki/Context_switch" target="_blank" rel="external">http://en.wikipedia.org/wiki/Context_switch</a></p>
<h2 id="Increased_Resource_Consumption"><a href="#Increased_Resource_Consumption" class="headerlink" title="Increased Resource Consumption"></a>Increased Resource Consumption</h2><h2 id="u589E_u52A0_u8D44_u6E90_u6D88_u8017"><a href="#u589E_u52A0_u8D44_u6E90_u6D88_u8017" class="headerlink" title="增加资源消耗"></a>增加资源消耗</h2><p>A thread needs some resources from computer in order to run.<br>一个线程需要从计算机中获取相应资源以便于能够运行起来。</p>
<p>Besides CPU time a thread needs some memory to keep its local stack.<br>除此之外，线程在获取到 CPU 资源之后需要额外的内存空间来存放它的本地栈。</p>
<p>It may also take up some resources inside the operating system needed to manage the thread.<br>线程或许也会在用来管理线程的操作系统内部占据一些资源。</p>
<p>Try creating a program that creates 100 thread that does nothing but wait, and see how much memory the application takes when running.<br>(你可以)试着创建一个包含100个不做任何事情且仅处于等待状态的线程的程序，然后观察一下该程序运行起来的时候它总共消耗多少内存。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://kylin27.github.io/2016/02/27/2016.2.27.java_multithreading_costs3/" data-id="cill04bb1001erkimrtlvdvw1" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/concurrency/">concurrency</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/java/">java</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/translation/">translation</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-2016.27.java_concurrency.Multithreading_benifits2" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2016/02/27/2016.27.java_concurrency.Multithreading_benifits2/" class="article-date">
  <time datetime="2016-02-26T16:00:00.000Z" itemprop="datePublished">2016-02-27</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2016/02/27/2016.27.java_concurrency.Multithreading_benifits2/">Multithreading Benefits [多线程带来的福利]</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>Descriptions:<br>java 并发文档翻译，文章链接地址<a href="http://tutorials.jenkov.com/java-concurrency/benefits.html" target="_blank" rel="external">http://tutorials.jenkov.com/java-concurrency/benefits.html</a></p>
<hr>
<p>The reason multithreading is still used in spite of its challenges is that multithreading can have several benifits.<br>虽然使用多线程编程有着诸多挑战，但是它却仍被广泛使用的原因是因为多线程本身具有着许多优势。</p>
<p>Some of these benefits are:<br>优势如下:</p>
<ul>
<li>Better resource utilization.</li>
<li><p>(多线程)对资源的利用率更高。</p>
</li>
<li><p>Simpler program design in some situations.</p>
</li>
<li><p>在某些特定场合可以简化程序的设计。</p>
</li>
<li><p>More responsive programs.</p>
</li>
<li>可以写出响应性更好的程序。 </li>
</ul>
<hr>
<h2 id="Better_resource_utilization"><a href="#Better_resource_utilization" class="headerlink" title="Better resource utilization"></a>Better resource utilization</h2><h2 id="u66F4_u597D_u7684_u8D44_u6E90_u5229_u7528_u7387"><a href="#u66F4_u597D_u7684_u8D44_u6E90_u5229_u7528_u7387" class="headerlink" title="更好的资源利用率"></a>更好的资源利用率</h2><p>Imagine an applicaton that reads and processes files from the local file system.<br>设想一下有从本地文件系统读入并执行文件这样一段程序。</p>
<p>Lets say that reading a file from disk takes 5 seconds and processing it takes 2 seconds. Processing two files then takes<br>假设如果从磁盘上面将文件加载到当前程序中需要 5s ，然后执行该文件需要 2s.<br>那么代码处理 2 份文件的所需要的时间统计如下所示:</p>
<pre><code>
  5 seconds reading file A
  2 seconds processing file A
  5 seconds reading file B
  2 seconds processing file B
  ************************
  14 seconds total
</code></pre>

<p>When reading the file from disk most of the CPU time is spent waiting for the disk to read the data.<br>当程序在执行从磁盘上读入文件的这段期间内 CPU 将大部分的时间花费在等待上。</p>
<p>The CPU is pretty much idle druing that time.<br>在等待的那段期间内，CPU 很有可能处于一种空闲状态。</p>
<p>It could be doing something else.<br>而在那段时间内 CPU 完全是可以做点其他的事情的。</p>
<p>By changing the order of the operations, the CPU could be better utilized. Look at this ordering:<br>通过修改执行操作的顺序，能够提升 CPU 的利用率。<br>可以参照如下的顺序:</p>
<pre><code>
  5 seconds reading file A
  5 seconds reading file B + 2 seconds processing file A
  2 seconds processing file B
  ********************
  12 seconds total
</code></pre>

<p>The CPU waits for the first file to be read.<br>CPU 在首次从磁盘读入文件的时候处于等待状态。</p>
<p>Then it starts the read of the second file.<br>然后，开始第二份文件的加载操作。</p>
<p>While the second file is being read, the CPU processes the first file.<br>当等待第二份文件从硬盘加载的期间内， CPU 会运行程序来处理读入第一份文件。</p>
<p>Remember, while waiting for the file to be read from disk, the CPU is mostly idel.<br>记住这一点，当 CPU 处于等待文件从磁盘加载文件的这段期间时， CPU 十有八九是处于空闲状态的。</p>
<p>In general, the CPU can be doing other things while waiting for IO.<br>就通常情况来说，当 CPU 等待 IO 操作的这段期间内完全可以做一些其他事情。</p>
<p>It doesn’t have to be disk IO.<br>也不一定非要是基于磁盘的 IO 操作。</p>
<p>It can be network IO as well, or input from a user at the machine.<br>还可以是基于网络的 IO 操作，或是来自于另外一台主机用户的输入数据。</p>
<p>Network and disk IO is often a lot slower than CPU’s and memory IO.<br>网络和磁盘上数据的读入写出速度通常要远远比 CPU 直接读写内存的速度要慢。</p>
<h2 id="Simpler_program_desgin_in_some_situations"><a href="#Simpler_program_desgin_in_some_situations" class="headerlink" title="Simpler program desgin in some situations"></a>Simpler program desgin in some situations</h2><h2 id="u5728_u7279_u5B9A_u573A_u5408_u66F4_u7B80_u6D01_u7684_u7A0B_u5E8F_u8BBE_u8BA1"><a href="#u5728_u7279_u5B9A_u573A_u5408_u66F4_u7B80_u6D01_u7684_u7A0B_u5E8F_u8BBE_u8BA1" class="headerlink" title="在特定场合更简洁的程序设计"></a>在特定场合更简洁的程序设计</h2><p>If you were to program the above ordering of reading and processing by hand in a singlethread applicaton, you would have to keep track of both the read and procesisng state of each file.<br>如果你曾试图将上述的代码按照读、执行的顺序来用单线程编写实现的话，你将不得不在每个线程中保存为每个文件的读取和执行的相关状态信息。</p>
<p>Instead you can start two threads that each just reads and processes a single file.<br>同样你也可以创建两个线程，让每个线程仅做读文件或是处理文件中的一种。</p>
<p>Each of these threads will be blocked while waiting for the disk to read its file.<br>每当线程等待从文件从磁盘中读取的时间段内都会陷入阻塞状态。</p>
<p>While waiting, other threads can use the CPU to process the parts of the file they have already read.<br>在该线程处于等待的过程中，其他的线程便会使用 CPU 资源来操作它已经从磁盘加载到内存的文件。</p>
<p>The result is, that the disk is kept busy at all times, reading from various files into memory. This results in a better utilization of both the disk and the CPU.<br>这样操作的结果便是可以保证磁盘一直处于忙碌状态，将不同的文件加载到内存中。 这种处理方法也可以提高使磁盘和 CPU 的资源利用率。</p>
<p>It is also easier to program, since each thread only has to keep track of a single file.<br>另一个好处便是是程序的代码编写起来更加的容易，因为每个线程仅仅需要保存单个文件的信息即可。</p>
<h2 id="More_responsive_programs"><a href="#More_responsive_programs" class="headerlink" title="More responsive programs"></a>More responsive programs</h2><h2 id="u5199_u51FA_u54CD_u5E94_u6027_u66F4_u597D_u7684_u7A0B_u5E8F"><a href="#u5199_u51FA_u54CD_u5E94_u6027_u66F4_u597D_u7684_u7A0B_u5E8F" class="headerlink" title="写出响应性更好的程序"></a>写出响应性更好的程序</h2><p>Another common goal for turning a singlethreaded application into a multithreaded application is to achieve a more responsive application.<br>将单线程应用程序改写成多线程应用程序的另一个目的便是获得应用程序更好的响应能力。</p>
<p>Imagine a server application that listens on some port for incoming requests, when a request is received, it handles the request and then goes back to listening.<br>假设在某个端口上监听是否有请求到来的服务器应用程序， 如果接到了一个请求的话，这个服务器应用程序首先会处理这个请求，然后等到处理请求之后才会返回去继续监听操作。</p>
<p>The server loop is sketched below.<br>关于上述服务器的程序概况描述如下.</p>
<pre><code>
while (server is active){
    listen for request
    process request
}
</code></pre>

<p>If the request takes a long time to process, no new client can send requests to the server for that duration.<br>如果服务器端执行的请求需要很长的一端时间，那么在那段时间内没有任何客户端能够成功的将自己的请求发送至服务器端。</p>
<p>An alternate design would be for the listening thread to pass the request to a worker thread, and return to listening immediately.<br>可以在设计上进行这样的变动: 创建一个用来将请求信息转发给处理请求的工作者线程这样的请求转发线程，每当转发执行结束便会立即恢复到监听的状态。</p>
<p>The worker thread will process the request and send a reply to the client.<br>而上述设计变动中所描述的工作者线程将会执行具体的请求，然后将执行结果回复给客户端。</p>
<p>This design is sketched below:<br>上述的设计概况可以描述如下:</p>
<pre><code>
while ( server is active){
    listen for request
    hand request to worker thread
}

</code></pre>

<p>This way the server thread will be back at listening sooner.<br>在这种设计模式下，服务器线程能够在响应请求连接之后很短的时间内回复到监听状态。</p>
<p>Thus more clients can send requests to the server.<br>因此能够满足让更多的客户端将请求发送给服务器端。</p>
<p>The server has become more responsive.<br>这样的服务器的响应性更好。</p>
<p>The same is true for desktop applications.<br>上述的道理对于桌面应用程序来说也说得通。</p>
<p>If you click a button that starts a long task, and the thread executing the task is the thread updating the windows, button etc., then the application will appear unresponsive while the task executes.<br>如果你通过点击一个按钮来启动一个长任务的话，并且该线程所执行任务是更新 windows 系统，不过在更新任务执行的时间段内该程序对于所有的操作都不会有任何响应操作。</p>
<p>Instead the task can be handed off to a worker thread.<br>如果我们将’更新’任务交给一个工作者线程来完成的话。</p>
<p>While the worker thread is busy with the task, the window thread is free to respond to other user requests.<br>那么即便是工作者线程处于繁忙的执行任务状态，窗口线程仍是处于空闲状态且能够立即响应来自其他用户的请求。</p>
<p>When the worker thread is done it signals the window thread.<br>当工作者线程完成了它的工作的话，它便会向窗口线程发送信号。</p>
<p>The window thread can then update the application windows with the result of the task.<br>(接收到信号的)窗口线程随后便可以根据工作线程执行的结果来更新窗口应用程序。</p>
<p>The program with the worker thread design will appear more responsive to the user.<br>如果在设计程序的时候为其增设工作者线程的话将会让程序对于用户而言更具有响应性。</p>
<p><a href="/">下一篇文章</a></p>
<p>end</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://kylin27.github.io/2016/02/27/2016.27.java_concurrency.Multithreading_benifits2/" data-id="cill04ban000trkimmgst56zb" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/concurrency/">concurrency</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/java/">java</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/translation/">translation</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-2016.2.27.java_concurrency.Multithreading Tutorial_1" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2016/02/27/2016.2.27.java_concurrency.Multithreading Tutorial_1/" class="article-date">
  <time datetime="2016-02-26T16:00:00.000Z" itemprop="datePublished">2016-02-27</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2016/02/27/2016.2.27.java_concurrency.Multithreading Tutorial_1/">Java Concurrency / Multithreading Tutorial [Java 并发/多线程教程]</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>Description:<br>java 并发文档翻译，文章链接地址<a href="http://tutorials.jenkov.com/java-concurrency/index.html" target="_blank" rel="external">http://tutorials.jenkov.com/java-concurrency/index.html</a></p>
<hr>
<p><br>Back in the old days a computer had a single CPU, and was only capable of executing a single program at a time.<br><br>回退到旧时光，那时的计算机只有单核并且每次只能够运行一段程序。<br><br><br>Later came multitasking which meant that computers could execute multiple programs(AKA tasks or processes) at the same time.<br><br>不久之后多任务计算机登场，多任务计算机指的是那些可以同时运行多段程序(也可以叫做多任务或多进程)的计算机。<br><br><br>It wasn’t really “at the same time” though.<br><br>事实上这些任务并非是在同一时间段”并发的”。<br><br><br>The single CPU was shared between the programs.<br><br>计算机单核的处理器资源是以共享的方式在多个程序之间被调用。<br><br><br>The operating system would switch between the programs running, executing each of them for a little while before switching.<br><br>操作系统也是在多个”同时”执行的程序间来回进行切换，在从一个程序切换到另一个程序的过程中依次执行每段程序中的一小段。<br><br></p>

<p><br>Along with multitasking came new challenges for software developers.<br><br>随着多任务计算机时代的来临，软件开发人员也面临着新的跳帧。<br><br><br>Programers can no longer assume to have all the CPU time available, nor all memory or any other computer resources.<br><br>程序员再也不能确保随时都能够访问计算机的 CPU ，内存乃至其他计算机的资源了。<br><br><br>A “good citizen” program should release all resources it is no longer using, so other programs can use them.<br><br>对于一段”模范”程序来说，应该将对它无用的资源全部释放掉，以确保这些资源可被其他程序所调用。<br><br></p>

<p><br>Later yet came multithread which mean that you could have multiple thread of execution inside the same program.<br>接下来到来的便是多线程的时代，就是在同一段程序中允许你能同时运行多个线程。<br><br><br>A thread of execution can be thought of as a CPU executing the program. <br><br>由一个线程所运行的程序可被等价地看做是由一个 CPU 所执行的程序。<br><br><br>When you have multiple threads executing the same program, it is like having multiple CPUs execute within the same program.<br><br>多个线程运行同一段程序，便可以将其比作是多个 CPU 在同一段程序中运行。<br><br></p>

<p><br>Multithreading can be a great way to increase the performance of some types of programs.<br><br>多线程这种技术可以极大程度上来提高某种类型程序的效率。<br><br><br>However, multithreading is even more challenging than multitasking. <br><br>然而，多线程编程要远比多任务编程面临更多的挑战。<br><br><br>The threads are executing within the same program and are hence reading and writing the same memory simultanously.<br><br>由于程序允许多个线程运行在其中，因此这些线程便有可能在同一时间点上的读写相同的内存空间。<br><br><br>This can result in errors not seen in a singlethread program.<br><br>上述的这种操作可能引发在单线程运行的程序中遇不到的错误的发生。<br><br><br>Some of these errors may not be seen on single CPU machines, because two threads never really execute “simultanously”.<br><br>在引发的错误中有些错误在单核计算机上可能不会遇到，因为运行于单核计算机上的两个线程并不是真正意义上的并发。<br><br><br>Modern computers, though, come with multicore CPUs, and even with multiple CPUs too. This means that separate threads can be executed by separate cores or CPUs simultaneously.<br><br>如今的现代计算机都是多核甚至多 CPU 的。 这就意味着可以将线程以 CPU 的核或是 CPU 为单位来实现并发计算。<br><br><br></p>

<p><img src="http://7xqz39.com1.z0.glb.clouddn.com/thread1_2345%E6%88%AA%E5%9B%BE20160227111114.png" alt=""></p>
<p><br>If a thread reads a memory location while another thread writes to it, what value will the first thread end up reading ? <br><br>(设想一下这个场景)如果一个线程在读取一个内存单元空间的时候恰好另一个线程在对这块内存空间执行写入操作的话，第一个线程在执行读操作之后它将获得的数值是什么？ <br><br><br>The old value ? <br><br>是内存空间中原先的那个数值？<br><br><br>The value written by the second thread ? <br><br>还是另一个线程刚刚写入的数值？<br><br><br>Or a value that is mix between the two ? <br><br>还是新旧混合的数值？<br><br><br>Or, if two threads are writing to be the same memory location simultanously, what value will be left when they are done ? <br><br>又或是，如果两个线程同时向同一块内存空间中执行写入操作的话，当这两个线程执行写操作结束的时候，所写入的内存空间中将会是那个数值？<br><br><br>The value written by the first thread ?<br><br>是第一个线程写入的数值？<br><br><br>The value written by the second thread ? <br><br>还是第二个线程刚刚写入的数值？<br><br><br>Or a mix of the two values written?<br><br>亦或是两个线程写入数值的混合值？<br><br></p>


<p><br>Without proper precautions any of these outcomes are possible.<br><br>如果没有适当的提前声明(约定)的话上述的任何一种情况都是有可能发生的。<br><br><br>The behavior would not even be predicable.<br><br>执行操作的方式甚至都有可能是不可预知的。<br><br><br>The outcome could change from time to time.<br><br>也有可能每次运行都会得到不同的结果。<br><br><br>Therefore it is important as a developer to know how to take the right precautions - meaning learning to control how threads access shared resources like memory, files, databases etc.<br><br>因此对于一个开发人员来说认识到如何(根据不同的情景/场合)来做出正确的预防措施 - 也就是说他应该清楚如何控制线程来(安全地)访问一些像是内存，文件，数据库等等诸如此类的共享资源 是十分重要的。<br><br><br>That is one of the topics this java concurrency tutorial addresses.<br><br>而(如何在多线程并发的情况下安全地访问共享资源)这便是这篇’java 并发教程’所要向您重点讲述的内容。<br><br></p>

<h2 id="Multithreading_and_Concurrency_in_Java"><a href="#Multithreading_and_Concurrency_in_Java" class="headerlink" title="Multithreading and Concurrency in Java"></a>Multithreading and Concurrency in Java</h2><h2 id="Java__u4E2D_u7684_u591A_u7EBF_u7A0B_u4E0E_u5E76_u53D1"><a href="#Java__u4E2D_u7684_u591A_u7EBF_u7A0B_u4E0E_u5E76_u53D1" class="headerlink" title="Java 中的多线程与并发"></a>Java 中的多线程与并发</h2><p><br>Java was one of the first languages to make multithreading easily available to developers. <br><br>Java 是第一批实现将多线程功能对开发者而言简单易用的开发语言中的一种。<br><br><br>Java had multithreading capabilities from the very beginging. <br><br>Java 这种开发语言在被创造之处就支持多线程编程这种技术。<br><br><br>Therefore, Java developers often face the problems described above. <br><br>这也是为何 Java 的开发人员经常会在开发中遇到上描述的众多问题。<br><br><br>That is the reason I am writing this trail on Java concurrency.<br><br>而这也正是为何我要编写以 Java 并行技术为主题的这一系列文档。<br><br><br>As notes to myself, and any fellow Java developer whom may benifit from it.<br><br>而这一系列文章也将作为我自己的学习笔记，同时希望可以让其他的 Java 开发人员从中受益。<br><br></p>

<p><br>The trail will primarily be concerned with multithreading in Java, but some of the problems occuring in multithreading are similar to problems occuring in multitasking and in distributed systems. <br><br><br>这篇专栏主要关注 Java 开发中的多线程技术，除此之外其实在多任务系统和分布式系统中也会遇到与多线程相似的问题这也是我们讨论的重点。<br><br><br>References to multitasking and distributed systems may therefore occur in this trail too.<br><br>考虑到多任务和分布式系统中的相关知识也会在本文中有所提及。 <br><br><br>Hence the word “concurrency” rather “multithreading”.<br><br>所以在这里我们将这一系列的文章以’并发’而不是’多线程’来进行命名。<br><br></p>


<h2 id="2015__u5E74_u4E4B_u540E_u548C_2015__u5E74_u4E4B_u524D_u7684_Java__u5E76_u53D1_u6280_u672F"><a href="#2015__u5E74_u4E4B_u540E_u548C_2015__u5E74_u4E4B_u524D_u7684_Java__u5E76_u53D1_u6280_u672F" class="headerlink" title="2015 年之后和 2015 年之前的 Java 并发技术"></a>2015 年之后和 2015 年之前的 Java 并发技术</h2><p><br>A lot has happened in the world of concurrent architecture and design since the first Java concurrency books were written, and even since the Java 5 concurrency utilities were released.<br><br>自从第一版关于 Java 并发的书籍问世以及 Java 5 中的并发包被发布之后，并发架构的世界中已经发生了很大的变动。<br><br></p>

<p><br>New, asynchronous “shared-nothing” platforms and APIs like Vert.x and Play/Akka and Qbit have emerged.<br><br>新的一种，异步的 “无需共享任何资源” 的平台和API 像是 Vert.x , Play/Akka 和 Qbit 这样的框架相继涌现。<br><br><br>These platforms use a different concurrency model than the standard Java/JEE concurrency model of threading, shared memory and locking. <br><br>这种平台相比于标准的 Java/JEE 使用共享内存和所的线程并发模式，采用了完全不同的并发模式。<br><br><br>New non-blocking concurrency algorithms have been published, and new non-blocking tools like the LMax Disrupter have been added to our toolkits. <br><br>新式的无阻塞并发算法被推出，同时新型的无阻塞开发工具显示 LMax Disrupter 被键入到我们的开发平台工具中。<br><br><br>New functional programming parallelism has been introduced with the Fork and Join framework in Java7, and the collection streams API in Java8.<br><br>基于函数式并发新的编程方法被引入到 Java7 中，同时在 Java8 中对集合流的相关 API 予以支持。<br><br></p>

<p><br>With all these new developments it is about time that I updated this Java Concurrency tutorial. <br><br>Java 开发中发生了如此之多的变化，所以这也是我该更新 Java 并发教程手册的时候了。<br><br><br>Therefore, this tutorial is once again work in progress. <br><br>这篇 Java 并发教程会一如既往地以不断更新的方式来发布。<br><br><br>New tutorials will be published whenever time is available to write them.<br><br>只要我一有时间便会向文章集中添加新的教程文章。<br><br></p>

<p><a href="/">下一篇文章</a></p>
<p>end</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://kylin27.github.io/2016/02/27/2016.2.27.java_concurrency.Multithreading Tutorial_1/" data-id="cill04bb3001irkimx0ibmntp" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/concurrency/">concurrency</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/java/">java</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/translate/">translate</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-2016.2.26.remote_debug_spark_intellij" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2016/02/26/2016.2.26.remote_debug_spark_intellij/" class="article-date">
  <time datetime="2016-02-25T16:00:00.000Z" itemprop="datePublished">2016-02-26</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2016/02/26/2016.2.26.remote_debug_spark_intellij/">基于单节点的 Spark &amp; IDEA 远程调试</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>Description:<br>本篇博文主要介绍一下，如何使用 intellij IDEA 这款可视化集成编程工具来远程调试运行在 Spark 集群上面的 jar 文件。<br>在这里采用伪分布式单节点来部署 Spark 的运行环境，目的是为了防止分布式集群环境中(创建多个虚拟机在每台虚拟机上面部署 Spark 环境)一些不可控错误的发生。</p>
<hr>
<h1 id="u73AF_u5883_u642D_u5EFA"><a href="#u73AF_u5883_u642D_u5EFA" class="headerlink" title="环境搭建"></a>环境搭建</h1><h2 id="u5B9E_u9A8C_u73AF_u5883_u51C6_u5907"><a href="#u5B9E_u9A8C_u73AF_u5883_u51C6_u5907" class="headerlink" title="实验环境准备"></a>实验环境准备</h2><ul>
<li>虚拟机 Oracle Virtual Box 版本：5.0.10</li>
<li>虚拟机中的 Linux 版本： 15.0.4</li>
<li>Linux 中安装的 spark 版本: <a href="http://pan.baidu.com/s/1o6P3TVk" target="_blank" rel="external">spark-1.5.2-bin-hadoop2.6.tgz</a></li>
<li>Linux 中安装的 hadoop 版本: <a href="http://pan.baidu.com/s/1o6P3TVk" target="_blank" rel="external">hadoop-2.6.3.tar.gz</a></li>
<li>Linux 中安装的 JDK 版本: 1.7.0</li>
<li>Linux 中安装的 scala 版本: 2.11.7</li>
<li>主机参数: <ul>
<li>CPU: i5-4460  </li>
<li>RAM: 8.00 GB</li>
</ul>
</li>
<li>主机 Intellij 版本: Intellij-14.14 (正版破解版… =。=) </li>
<li>主机 JDK 版本: 1.8.0 </li>
<li>主机 spark 软件发布包:<a href="http://pan.baidu.com/s/1o6P3TVk" target="_blank" rel="external">spark-1.5.2-bin-hadoop2.6.tgz</a></li>
<li><h2 id="u5728_u865A_u62DF_u673A_u4E0A_u90E8_u7F72_u73AF_u5883"><a href="#u5728_u865A_u62DF_u673A_u4E0A_u90E8_u7F72_u73AF_u5883" class="headerlink" title="在虚拟机上部署环境"></a>在虚拟机上部署环境</h2></li>
<li>首先更新 Ubuntu 的源，然后安装 Oracle 版本的 java7 </li>
<li>安装过 spark 的同学告诉我，说是用 java8 来安装 spark 会抛出莫名异常，所以在 Linux 的上用的是就是 java7 版本，而在主机上 Intellij 的编译器使用的是 java8</li>
</ul>
<h3 id="u5355_u70B9_u90E8_u7F72_u5B89_u88C5_hadoop"><a href="#u5355_u70B9_u90E8_u7F72_u5B89_u88C5_hadoop" class="headerlink" title="单点部署安装 hadoop"></a>单点部署安装 hadoop</h3><ul>
<li>将 hadoop 压缩包解压到指定目录下面</li>
</ul>
<pre><code>
$ tar -xzf hadoop-2.6.3.tar.gz 
$ mkdir /hadoop
$ mv hadoop-2.6.3 /hadoop/
</code></pre>

<ul>
<li>为当前结点配置 ssh 的密钥对(即便是单点部署也需要实现 ssh 无密码登陆)</li>
</ul>
<pre><code>
$ apt-get install openssh-server            
* 如果系统中没有安装 ssh 软件的话使用 apt-get install 命令安装

$ apt-get install openssh-client        
* 安装 ssh 的客户端

$ ssh-keygen -t rsa -P ""                 
* 生成无密码的公私密钥对    

$ cd ~/.ssh/                            
* 生成的密钥对文件存放在 ~/.ssh/ 文件夹的下面

$ cat id_rsa.pub >> authorized_keys     
* 将刚刚生成的公钥文件中的内容以追加的方式写入到 authorized_keys 文件中

$ chmod 600 authorized_keys             
* 修改存放公钥文件的权限

$ ssh localhost                            
* 使用 ssh 来远程登录自身所在的主机

$ hostname                               
* 查看主机名称，我的是 aimer-v，存放主机名的配置文件(Ubuntu)是 /etc/hostname

$ ssh aimer-v                             
* 使用 ssh 登录名为 aimer-v 的主机，如果能够成功登录说明 ssh 正常工作

* 如果出现无法 ssh 正常访问登录的情况的话，首先使用 ping 命令来检查数据包是否可以正常收发
* 如果 ping 没有问题的话同时修改 ssh 对应路径下面的配置文件，并通过重启 ssh 服务来是配置文件生效
* 我的 ssh 配置文件所在路径是

$ /etc/ssh/ssh_config

* 修改过(注销注释)的字段是
   PasswordAuthentication yes
   CheckHostIP yes
   IdentityFile ~/.ssh/id_rsa
   Port 22
   SendEnv LANG LC_*
   HashKnownHosts yes
   GSSAPIAuthentication yes
   GSSAPIDelegateCredentials no

</code></pre>

<ul>
<li>修改系统环境配置文件将 Hadoop 相关路径写入存放到其中,也就是将 Hadoop 安装包所在的文件夹路径添加到系统的搜索路径中，以便于用户无论在那个路径下面输入 Hadoop 相关命令都可以启动运行 Hadoop 文件夹下面的可执行脚本</li>
</ul>
<pre><code>
$　vi /etc/profile            
 在文件的末尾追加

#set for hadoop
export HADOOP_HOME=/hadoop
export PATH=$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$PATH
export HADOOP_MAPRED_HOME=$HADOP_HOME
export HADOOP_COMMON_HOME=$HADOOP_HOME
export YARN_HOME=$HADOOP_HOME
export HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_HOME/lib/native
export HADOOP_OPTS="-Djava.library.path=$HADOOP_HOME/lib:$HADOOP_HOME/lib/native"

$ source /etc/profile       # 让修改过的系统变量立即生效
</code></pre>

<ul>
<li>修改 Hadoop 的相关配置文件<br>Hadoop 的一系列配置文件所在路径为 ${HADOOP_HOME}/etc/hadoop/ 的下面 </li>
</ul>
<ul>
<li>首先修改的是名为 hadoop-env.sh 的配置文件</li>
</ul>
<pre><code>
  # hadoop-env.sh 文件中记录的是 hadoop 在启动的时候，到哪里去找 java 的编译器
  # 和启动时内存空间大小的分配等，我只修改了下面这一个选项 
  export JAVA_HOME=/usr/lib/jvm/java-7-oracle    
</code></pre>  

<ul>
<li>然后修改的是名为 core-site.xml 的配置文件</li>
</ul>
<p><img src="http://7xqz39.com1.z0.glb.clouddn.com/tutorial-core-xml.png" alt=""></p>
<ul>
<li>接下来修改的是 hdfs-site.xml </li>
</ul>
<p><img src="http://7xqz39.com1.z0.glb.clouddn.com/tutorial-hdfs-xml.png" alt=""> </p>
<ul>
<li>最后修改的是 yarn-site.xml<br>yarn 是 apache 的资源管理调度框架，和 yarn 等价的还有 mesos ，我没有在这里做过深入研究，感兴趣的同学可以进一步查阅相关资料。</li>
</ul>
<p><img src="http://7xqz39.com1.z0.glb.clouddn.com/tutorial-yarn-xml.png" alt=""></p>
<ul>
<li>根据配置文件中设置的文件夹和文件，并在当前系统中的对应路径下创建对应的文件夹和文件</li>
</ul>
<pre><code>
$ mkdir /hadoop/tmp
$ mkdir /hadoop/dfs
$ mkdir /hadoop/dfs/name
$ mkdir /hadoop/dfs/data
</code></pre>

<ul>
<li>格式化 hdfs </li>
</ul>
<pre><code>
// 首先将路径切换到 ${HADOOP_HOME}/bin 的下面，然后执行下面的命令
$ ./hdfs namenode -format                  # 将 namenode 进行格式化操作

//  如果配置信息无误且正确运行的话，将会显示如下的输出信息 


STARTUP_MSG:   java = 1.7.0_80
************************************************************/
16/02/26 14:33:49 INFO namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]
16/02/26 14:33:49 INFO namenode.NameNode: createNameNode [-format]
Formatting using clusterid: CID-88b14724-7d23-46fd-a623-83029ad20c44
16/02/26 14:33:51 INFO namenode.FSNamesystem: No KeyProvider found.
16/02/26 14:33:51 INFO namenode.FSNamesystem: fsLock is fair:true
16/02/26 14:33:51 INFO blockmanagement.DatanodeManager: dfs.block.invalidate.limit=1000
16/02/26 14:33:51 INFO blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
16/02/26 14:33:51 INFO blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
16/02/26 14:33:51 INFO blockmanagement.BlockManager: The block deletion will start around 2016 Feb 26 14:33:51
16/02/26 14:33:51 INFO util.GSet: Computing capacity for map BlocksMap
16/02/26 14:33:51 INFO util.GSet: VM type       = 64-bit
16/02/26 14:33:51 INFO util.GSet: 2.0% max memory 966.7 MB = 19.3 MB
16/02/26 14:33:51 INFO util.GSet: capacity      = 2^21 = 2097152 entries
16/02/26 14:33:51 INFO blockmanagement.BlockManager: dfs.block.access.token.enable=false
16/02/26 14:33:51 INFO blockmanagement.BlockManager: defaultReplication         = 1
16/02/26 14:33:51 INFO blockmanagement.BlockManager: maxReplication             = 512
16/02/26 14:33:51 INFO blockmanagement.BlockManager: minReplication             = 1
16/02/26 14:33:51 INFO blockmanagement.BlockManager: maxReplicationStreams      = 2
16/02/26 14:33:51 INFO blockmanagement.BlockManager: replicationRecheckInterval = 3000
16/02/26 14:33:51 INFO blockmanagement.BlockManager: encryptDataTransfer        = false
16/02/26 14:33:51 INFO blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
16/02/26 14:33:51 INFO namenode.FSNamesystem: fsOwner             = root (auth:SIMPLE)
16/02/26 14:33:51 INFO namenode.FSNamesystem: supergroup          = supergroup
16/02/26 14:33:51 INFO namenode.FSNamesystem: isPermissionEnabled = false
16/02/26 14:33:51 INFO namenode.FSNamesystem: HA Enabled: false
16/02/26 14:33:51 INFO namenode.FSNamesystem: Append Enabled: true
16/02/26 14:33:52 INFO util.GSet: Computing capacity for map INodeMap
16/02/26 14:33:52 INFO util.GSet: VM type       = 64-bit
16/02/26 14:33:52 INFO util.GSet: 1.0% max memory 966.7 MB = 9.7 MB
16/02/26 14:33:52 INFO util.GSet: capacity      = 2^20 = 1048576 entries
16/02/26 14:33:52 INFO namenode.NameNode: Caching file names occuring more than 10 times
16/02/26 14:33:52 INFO util.GSet: Computing capacity for map cachedBlocks
16/02/26 14:33:52 INFO util.GSet: VM type       = 64-bit
16/02/26 14:33:52 INFO util.GSet: 0.25% max memory 966.7 MB = 2.4 MB
16/02/26 14:33:52 INFO util.GSet: capacity      = 2^18 = 262144 entries
16/02/26 14:33:52 INFO namenode.FSNamesystem: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
16/02/26 14:33:52 INFO namenode.FSNamesystem: dfs.namenode.safemode.min.datanodes = 0
16/02/26 14:33:52 INFO namenode.FSNamesystem: dfs.namenode.safemode.extension     = 30000
16/02/26 14:33:52 INFO namenode.FSNamesystem: Retry cache on namenode is enabled
16/02/26 14:33:52 INFO namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
16/02/26 14:33:52 INFO util.GSet: Computing capacity for map NameNodeRetryCache
16/02/26 14:33:52 INFO util.GSet: VM type       = 64-bit
16/02/26 14:33:52 INFO util.GSet: 0.029999999329447746% max memory 966.7 MB = 297.0 KB
16/02/26 14:33:52 INFO util.GSet: capacity      = 2^15 = 32768 entries
16/02/26 14:33:52 INFO namenode.NNConf: ACLs enabled? false
16/02/26 14:33:52 INFO namenode.NNConf: XAttrs enabled? true
16/02/26 14:33:52 INFO namenode.NNConf: Maximum size of an xattr: 16384
Re-format filesystem in Storage Directory /hadoop/dfs/name ? (Y or N) 

//  输入 'Y' 表示同意格式化 namenode 
//  如果成功初始化的话，将会显示如下的信息 

16/02/26 14:34:55 INFO namenode.FSImage: Allocated new BlockPoolId: BP-1525144641-127.0.0.1-1456468495698
16/02/26 14:34:56 INFO common.Storage: Storage directory /hadoop/dfs/name has been successfully formatted.
16/02/26 14:34:56 INFO namenode.NNStorageRetentionManager: Going to retain 1 images with txid >= 0
16/02/26 14:34:56 INFO util.ExitUtil: Exiting with status 0
16/02/26 14:34:56 INFO namenode.NameNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down NameNode at aimer-v/127.0.0.1
************************************************************/

显示如上信息便说明 namenode 格式化成功 
</code></pre>


<ul>
<li>然后再将路径切换到 ${HADOOP_HOME}/sbin 的下面</li>
</ul>
<pre><code>
$ ./start-dfs.sh                           # 启动 hdfs 
</code></pre>

<ul>
<li>如若成功启动显示日志信息如下 </li>
</ul>
<pre><code>
root@aimer-v:/hadoop/sbin# ./start-dfs.sh 
Starting namenodes on [localhost]
localhost: starting namenode, logging to /hadoop/logs/hadoop-root-namenode-aimer-v.out
localhost: starting datanode, logging to /hadoop/logs/hadoop-root-datanode-aimer-v.out
Starting secondary namenodes [0.0.0.0]
0.0.0.0: starting secondarynamenode, logging to /hadoop/logs/hadoop-root-secondarynamenode-aimer-v.out
</code></pre>

<ul>
<li>启动 yarn </li>
</ul>
<pre><code>
$ ./start-yarn.sh                          
</code></pre>

<ul>
<li>若 yarn 成功运行显示如下日志信息 </li>
</ul>
<pre><code>
root@aimer-v:/hadoop/sbin# ./start-yarn.sh  &
[1] 5993
root@aimer-v:/hadoop/sbin# starting yarn daemons
starting resourcemanager, logging to /hadoop/logs/yarn-root-resourcemanager-aimer-v.out
localhost: starting nodemanager, logging to /hadoop/logs/yarn-root-nodemanager-aimer-v.out

[1]+  Done                    ./start-yarn.sh
</code></pre>

<ul>
<li>通过输入 jps 命令来查看 Hadoop 相关进程是否处于正常工作的状态</li>
</ul>
<pre><code>
$ jps 
5602 NameNode
6137 NodeManager
5866 SecondaryNameNode
6031 ResourceManager
6445 Jps
</code></pre>

<h3 id="u5B89_u88C5_spark__u4E4B_u524D_u5148_u5B89_u88C5_u597D_u548C_u5B89_u88C5_spark__u7248_u672C_u76F8_u5339_u914D_u7684_scala"><a href="#u5B89_u88C5_spark__u4E4B_u524D_u5148_u5B89_u88C5_u597D_u548C_u5B89_u88C5_spark__u7248_u672C_u76F8_u5339_u914D_u7684_scala" class="headerlink" title="安装 spark 之前先安装好和安装 spark 版本相匹配的 scala"></a>安装 spark 之前先安装好和安装 spark 版本相匹配的 scala</h3><ul>
<li>首先写在系统中默认安装的 scala</li>
</ul>
<pre><code>
$ apt-get remove scala 
</code></pre>

<ul>
<li>将下载到本地的 <a href="http://pan.baidu.com/s/1o6P3TVk" target="_blank" rel="external">scala</a> 压缩包进行解压</li>
</ul>
<pre><code>
$ tar -xvf scala-2.11.7.tgz
</code></pre>

<ul>
<li>修改系统配置文件，将 scala 所在路径追加到系统搜索路径中</li>
</ul>
<pre><code>
$ vi /etc/profile
</code></pre>

<ul>
<li>向文件中追加如下的信息 </li>
</ul>
<pre><code>
export SCALA_HOME=/scala
export PATH=$SCALA_HOME/bin:$PATH
</code></pre>

<h3 id="u5728_hadoop__u7684_u57FA_u7840_u4E0A_u7EE7_u7EED_u5B89_u88C5_spark"><a href="#u5728_hadoop__u7684_u57FA_u7840_u4E0A_u7EE7_u7EED_u5B89_u88C5_spark" class="headerlink" title="在 hadoop 的基础上继续安装 spark"></a>在 hadoop 的基础上继续安装 spark</h3><ul>
<li>解压软件包</li>
</ul>
<pre><code>
tar -xvf spark-1.5.2-bin-hadoop2.6.tgz
</code></pre>

<ul>
<li>修改系统配置文件，将 spark 所在路径添加到系统搜索路径中</li>
</ul>
<figure class="highlight xquery"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">$ vi /etc/profile </span><br><span class="line">export HADOOP_CONF_DIR=/hadoop/etc/hadoop</span><br><span class="line">export SPARK_MASTER=localhost</span><br><span class="line">export SPARK_LOCAL_IP=localhost</span><br><span class="line">export SPARK_HOME=/spark</span><br><span class="line">export SPARK_LIBRARY_PATH=.:<span class="variable">$JAVA</span>_HOME/lib:<span class="variable">$JAVA</span>_HOME/jre/lib:<span class="variable">$HADOOP</span>_HOME/lib/native</span><br><span class="line">export YARN_CONF_DIR=/hadoop/etc/hadoop</span><br><span class="line">export PATH=<span class="variable">$PATH</span>:<span class="variable">$SPARK</span>_HOME/bin</span><br><span class="line">export SCALA_HOME=/opt/scala</span><br><span class="line">export PATH=<span class="variable">$SCALA</span>_HOME/bin:<span class="variable">$PATH</span></span><br></pre></td></tr></table></figure>
<ul>
<li>最后通过该命令让修改的系统配置信息立即生效</li>
</ul>
<figure class="highlight gradle"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="keyword">source</span> <span class="regexp">/etc/</span>profile</span><br></pre></td></tr></table></figure>
<ul>
<li>修改 spark 的配置文件</li>
</ul>
<pre><code>
$ cd ${SPARK_HOME}/conf
$ cp spark-env.sh.template spark-env.sh      # 在这里建议将配置文件的文件模板进行保留，通过创建它的备份的方式来在备份文件上面进行修改
</code></pre>

<ul>
<li>打开 spark-env.sh 文件，然后添加如下的信息</li>
</ul>
<pre><code>
$ vi spark-env.sh                    

export JAVA_HOME=/usr/lib/jvm/java-7-oracle
export SCALA_HOME=/opt/scala
export HADOOP_CONF_DIR=/hadoop/etc/hadoop
</code></pre>


* 通过脚本来启动 spark 相关的服务

<pre><code>
$ cd ${SPARK_HOME}/sbin
$ ./start-all.sh
</code></pre>

<ul>
<li>如果启动成功的话，将会显示出如下的信息 </li>
</ul>
<pre><code>
root@aimer-v:/spark/sbin# ./start-all.sh 
rsync from localhost
rsync: change_dir "/spark/sbin//localhost" failed: No such file or directory (2)
rsync error: some files/attrs were not transferred (see previous errors) (code 23) at main.c(1183) [sender=3.1.1]
starting org.apache.spark.deploy.master.Master, logging to /spark/sbin/../logs/spark-root-org.apache.spark.deploy.master.Master-1-aimer-v.out
localhost: starting org.apache.spark.deploy.worker.Worker, logging to /spark/sbin/../logs/spark-root-org.apache.spark.deploy.worker.Worker-1-aimer-v.out

</code></pre>


<ul>
<li>通过输入 jps 命令来查看系统中各个进程的运行状态信息</li>
</ul>
<pre><code>
$ jps

root@aimer-v:/spark/sbin# jps
5470 SecondaryNameNode
5332 DataNode
6911 Worker
6690 Master
5776 NodeManager
5224 NameNode
5669 ResourceManager
6955 Jps
</code></pre>

<ul>
<li>其中的 Master 和 Worker 便是我们刚才启动 Spark 所运行的相关进程</li>
</ul>
<ul>
<li>通过脚本来运行 spark-shell 通过脚本的方式来访问 spark </li>
</ul>
<pre><code>
$ cd  ${SPARK_HOME}/bin
$ ./spark-shell
//  如果正常启动的话，将会显示如下日志信息 
</code></pre>

<p><img src="http://7xqz39.com1.z0.glb.clouddn.com/tutorial-spark_run.png" alt=""></p>
<p>自此，虚拟机上面 hadoop &amp; spark 的单节点运行环境部署结束 </p>
<hr>
<h2 id="u5728_u4E3B_u673A_u4E0A_u90E8_u7F72_u73AF_u5883"><a href="#u5728_u4E3B_u673A_u4E0A_u90E8_u7F72_u73AF_u5883" class="headerlink" title="在主机上部署环境"></a>在主机上部署环境</h2><h3 id="u642D_u5EFA_WordCounter__u7684_u7F16_u7A0B_u73AF_u5883"><a href="#u642D_u5EFA_WordCounter__u7684_u7F16_u7A0B_u73AF_u5883" class="headerlink" title="搭建 WordCounter 的编程环境"></a>搭建 WordCounter 的编程环境</h3><ul>
<li><p>在这里我们使用的是 scala 编程语言来进行编写 wordcounter 程序  </p>
</li>
<li><p>step 1. 在 Intellij 中创建一个新的 scala 项目</p>
</li>
</ul>
<p><img src="http://7xqz39.com1.z0.glb.clouddn.com/1_.png" alt=""><br><img src="http://7xqz39.com1.z0.glb.clouddn.com/2_.png" alt=""></p>
<ul>
<li><p>step 2. 打开 File -&gt; Project Structure -&gt; 点击最左栏中的 Libraries 选项 –&gt; 绿色的 ‘+’ 按钮</p>
</li>
<li><p>step 3. 将刚刚下载的 spark-1.5.2-bin-hadoop2.6 文件下 spark-1.5.2-bin-hadoop2.6\spark-1.5.2-bin-hadoop2.6\lib\spark-assembly-1.5.2-hadoop2.6.0.jar 文件加载到当前编程环境中</p>
</li>
</ul>
<p><img src="http://7xqz39.com1.z0.glb.clouddn.com/3_.png" alt=""></p>
<ul>
<li>step 4. 设置编译代码生成的 .jar 文件所在的路径， 打开 File -&gt; Project Structure -&gt; 点击最左栏中的 Artifacts 选项 –&gt; 绿色的 ‘+’ 按钮 Jar -&gt; From modules with dependencies ，然后在弹出的 ‘Create JAR from Modules’ 中的 ‘Main Class’ 选中对应的函数入口类文件，在这里我们选的是 SparkWordCount 这个文件</li>
<li></li>
</ul>
<p><img src="http://7xqz39.com1.z0.glb.clouddn.com/4_.png" alt=""></p>
<ul>
<li>step 5. 为 META-INF/MAINFEST.MF 这个将要生成的配置文件设置路径</li>
<li>step 6. 在 Name 栏中设置将要生成的 .jar 文件的名称， 在 Output directory 一栏中设置 .jar 文件将会输出的路径</li>
<li>step 7. 同时不要忘了将 Build on make 设置有效，最后点击 ok 按钮</li>
<li></li>
</ul>
<p><img src="http://7xqz39.com1.z0.glb.clouddn.com/7_final_jar.png" alt=""></p>
<ul>
<li>step 8. 编写代码 SparkWordCount.scala 代码如下所示</li>
</ul>
<pre><code>
import org.apache.spark.{SparkConf, SparkContext}

object SparkWordCount {
  def main(args: Array[String]) {
    val conf = new SparkConf().setAppName("SparkWordCount").setMaster("localhost")
    val sc = new SparkContext(conf)
    val count=sc.textFile(args(0)).filter(line => line.contains("Spark")).count()
    println("count="+count)
    sc.stop()
  }
}
</code></pre>

<ul>
<li>在再次运行代码的过程中出了一点问题: IDE 报错了显示缺少 scala （SDK） 相关的 jar 文件</li>
<li>引发报错的原因是: 之前引用到当前系统中的 SDK 索引没有更新，重新导入一次即可，如下图所示, IDE 重新创建一下索引即可征程编译</li>
</ul>
<p><img src="http://7xqz39.com1.z0.glb.clouddn.com/6_.png" alt=""></p>
<p><img src="http://7xqz39.com1.z0.glb.clouddn.com/build.png" alt=""></p>
<ul>
<li>step 9. 编译刚刚编写的代码 Build -&gt; Make Project ， 然后到对应的路径下面找 jar 文件</li>
</ul>
<p><img src="http://7xqz39.com1.z0.glb.clouddn.com/result_8.png" alt=""></p>
<h3 id="u5C06_u751F_u6210_u7684_jar__u6587_u4EF6_u4E0A_u4F20_u81F3_u5B89_u88C5_u6709_spark__u7684_u865A_u62DF_u673A_u4E0A"><a href="#u5C06_u751F_u6210_u7684_jar__u6587_u4EF6_u4E0A_u4F20_u81F3_u5B89_u88C5_u6709_spark__u7684_u865A_u62DF_u673A_u4E0A" class="headerlink" title="将生成的 jar 文件上传至安装有 spark 的虚拟机上"></a>将生成的 jar 文件上传至安装有 spark 的虚拟机上</h3><ul>
<li>在对应的路径下创建文件夹，然后将生成的 .jar 文件上传到该文件夹下面</li>
</ul>
<p><img src="http://7xqz39.com1.z0.glb.clouddn.com/upload_9.png" alt=""></p>
<h2 id="u5F00_u59CB_u8FDC_u7A0B_u8C03_u8BD5"><a href="#u5F00_u59CB_u8FDC_u7A0B_u8C03_u8BD5" class="headerlink" title="开始远程调试"></a>开始远程调试</h2><h3 id="u9996_u5148_u5728_u865A_u62DF_u673A_28Linux_29_u7684_u547D_u4EE4_u884C_u4E2D_u8F93_u5165_u547D_u4EE4_u8BA9_spark__u6765_u4EE5_u8C03_u8BD5_u7684_u65B9_u5F0F_u6267_u884C_u4E0A_u4F20_jar__u6587_u4EF6"><a href="#u9996_u5148_u5728_u865A_u62DF_u673A_28Linux_29_u7684_u547D_u4EE4_u884C_u4E2D_u8F93_u5165_u547D_u4EE4_u8BA9_spark__u6765_u4EE5_u8C03_u8BD5_u7684_u65B9_u5F0F_u6267_u884C_u4E0A_u4F20_jar__u6587_u4EF6" class="headerlink" title="首先在虚拟机(Linux)的命令行中输入命令让 spark 来以调试的方式执行上传 jar 文件"></a>首先在虚拟机(Linux)的命令行中输入命令让 spark 来以调试的方式执行上传 jar 文件</h3><pre><code>
$ cd ${SPARK_HOME}/bin
</code></pre> 

<ul>
<li>然后查看虚拟机的 IP 地址信息，我的是 </li>
</ul>
<pre><code>
root@aimer-v:/home/aimer/spark_remote# ifconfig
eth0      Link encap:Ethernet  HWaddr 08:00:27:f5:3e:29  
          inet addr:10.0.2.15  Bcast:10.0.2.255  Mask:255.255.255.0
          inet6 addr: fe80::a00:27ff:fef5:3e29/64 Scope:Link
          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1
          RX packets:169490 errors:0 dropped:0 overruns:0 frame:0
          TX packets:50036 errors:0 dropped:0 overruns:0 carrier:0
          collisions:0 txqueuelen:1000 
          RX bytes:208071948 (208.0 MB)  TX bytes:3995229 (3.9 MB)

eth1      Link encap:Ethernet  HWaddr 08:00:27:e5:6b:20  
          inet addr:192.168.56.113  Bcast:192.168.56.255  Mask:255.255.255.0
          inet6 addr: fe80::a00:27ff:fee5:6b20/64 Scope:Link
          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1
          RX packets:934783 errors:0 dropped:0 overruns:0 frame:0
          TX packets:458868 errors:0 dropped:0 overruns:0 carrier:0
          collisions:0 txqueuelen:1000 
          RX bytes:987944856 (987.9 MB)  TX bytes:42138914 (42.1 MB)

eth2      Link encap:Ethernet  HWaddr 08:00:27:15:4d:1b  
          inet addr:192.168.56.112  Bcast:192.168.56.255  Mask:255.255.255.0
          inet6 addr: fe80::a00:27ff:fe15:4d1b/64 Scope:Link
          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1
          RX packets:390 errors:0 dropped:0 overruns:0 frame:0
          TX packets:840 errors:0 dropped:0 overruns:0 carrier:0
          collisions:0 txqueuelen:1000 
          RX bytes:45543 (45.5 KB)  TX bytes:103389 (103.3 KB)

lo        Link encap:Local Loopback  
          inet addr:127.0.0.1  Mask:255.0.0.0
          inet6 addr: ::1/128 Scope:Host
          UP LOOPBACK RUNNING  MTU:65536  Metric:1
          RX packets:965869 errors:0 dropped:0 overruns:0 frame:0
          TX packets:965869 errors:0 dropped:0 overruns:0 carrier:0
          collisions:0 txqueuelen:0 
          RX bytes:497637820 (497.6 MB)  TX bytes:497637820 (497.6 MB)
</code></pre>

<h3 id="u5728_u4E0A_u8FF0_u7684_IP__u5730_u5740_u4E2D"><a href="#u5728_u4E0A_u8FF0_u7684_IP__u5730_u5740_u4E2D" class="headerlink" title="在上述的 IP 地址中"></a>在上述的 IP 地址中</h3><ul>
<li>第一个是我用来在虚拟机上面以 NAT 的方式登录互联网的 IP </li>
<li>第二个 IP 地址是多结点分布式部署 spark 集群设定的 IP</li>
<li>第三个 IP 地址是用来进行主机到虚拟机二者之间进行 ssh 远程连接的 IP 地址</li>
<li><p>第四个 主机自循环 IP 地址</p>
</li>
<li><p>因为我们在进行远程调试的时候，是想把虚拟机中的调试信息数据通过端口号传到主机(windows)的上面，</p>
</li>
<li><p>所以选用的输出调试信息的 IP 地址与 ssh 所使用的相同,只不过端口号不同一个是 8888 另一个是 22 罢了</p>
</li>
<li><p>接下来，不要着急运行 jar ，在这里由于 word-counter 这个程序是从 hdfs 上面来读取文本文件的，</p>
</li>
<li><p>所以还需要将输入文本文件上传到 hdfs 的上</p>
</li>
<li><p>首先将本地的文本文件 README.md(我用的是 spark 的 README 文件，随便什么 ASCII 编码的文本文件都可以) 上传到 hdfs 的上面</p>
</li>
</ul>
<pre><code>
$ hdfs dfs -put REAEME.md /
</code></pre>

<ul>
<li>查看文件是否被正确的上传，以及对应的结果路径是否正确的被创建 (在此期间，发现 datanode 没有启动，所以 README.md 这个本地文件并没有正确上传，所以先停掉了 spark ,hadoop , 然后重启 hadoop ，在 hadoop 启动之后又将 spark 进行启动)</li>
</ul>
<pre><code>
$ hdfs dfs -ls /
root@aimer-v:/home/aimer/spark_remote# hdfs dfs -ls /
Found 5 items
-rw-r--r--   1 root supergroup       3593 2016-02-26 16:34 /README.md
drwx-wx-wx   - root supergroup          0 2016-01-01 23:31 /tmp
drwxr-xr-x   - root supergroup          0 2016-01-01 23:53 /user
</code></pre>

<ul>
<li>在输入文件上传，输出数据文件路径分别在 hdfs 上创建好之后便可以输入如下命令</li>
</ul>
<pre><code>
$ ./spark-submit --master spark://aimer-v:7077 --name SparkWordCount --class SparkWordCount --driver-java-options "-Xdebug -Xrunjdwp:transport=dt_socket,server=y,suspend=y,address=8888" --executor-memory 1G /home/aimer/spark_remote/spark_learning.jar hdfs://aimer-v:9000/README.md
</code></pre>

<ul>
<li><p>上述的 –driver-java-options 后面所跟随的命令参数是是这样的: </p>
<ul>
<li>-Xdebug </li>
<li><p>这个参数是通知 JVM 工作在 DEBUG 的模式下面</p>
</li>
<li><p>-Xrunjdwp 这个参数是用来通知 JVM 使用 (java debug wire protocol) 来运行调试环境</p>
</li>
<li><p>-Xrunjdwp:transport=dt_socket 这个参数用来指定的是调试期间生成的数据传输的方式，</p>
</li>
<li><p>如果后跟 dt_shmem 这个参数的话表示的是以共享内存的方式来传递调试产生的数据，不过 <code>dt_shmem</code> 仅在 Windows 平台下适用。</p>
</li>
<li><p>server 该参数指的是是否支持在 server 模式的 VM 中</p>
</li>
<li><p>suspend 参数是用来设定是否等到用于调试的客户端成功创建连接之后，再来执行 JVM</p>
</li>
<li><p>address 参数用来指定的是调试信息发送的端口号</p>
</li>
</ul>
</li>
</ul>
<ul>
<li>如果正确运行的话，命令行中会显示如下的信息 </li>
</ul>
<pre><code>
Listening for transport dt_socket at address: 8888
</code></pre>

<h3 id="u7136_u540E_u5728_u672C_u5730_u7684_Intellij__u4E2D_u901A_u8FC7_u5982_u4E0B_u7684_u914D_u7F6E_u6765_u63A5_u6536_u8FDC_u7A0B_u53D1_u6765_u7684_u8C03_u8BD5_u4FE1_u606F_uFF0C_u5B9E_u73B0_u8FDC_u7A0B_u8C03_u8BD5_u529F_u80FD"><a href="#u7136_u540E_u5728_u672C_u5730_u7684_Intellij__u4E2D_u901A_u8FC7_u5982_u4E0B_u7684_u914D_u7F6E_u6765_u63A5_u6536_u8FDC_u7A0B_u53D1_u6765_u7684_u8C03_u8BD5_u4FE1_u606F_uFF0C_u5B9E_u73B0_u8FDC_u7A0B_u8C03_u8BD5_u529F_u80FD" class="headerlink" title="然后在本地的 Intellij 中通过如下的配置来接收远程发来的调试信息，实现远程调试功能"></a>然后在本地的 Intellij 中通过如下的配置来接收远程发来的调试信息，实现远程调试功能</h3><ul>
<li>step 1 Run -&gt; Edit Configurations 配置如下图所示的远程调试配置信息,为这个创建的远程调试设定一个名称 “remote-spark”</li>
</ul>
<p><img src="http://7xqz39.com1.z0.glb.clouddn.com/%E8%BF%9C%E7%A8%8B%E8%B0%83%E8%AF%95%E9%85%8D%E7%BD%AE%E4%BF%A1%E6%81%AF.png" alt=""></p>
<ul>
<li>step 2 Run -&gt; Debug ‘remote-spark’<br>如果成功连接的话，将会在下面显示如下的信息(不要忘了在本地的代码上打上端点)</li>
</ul>
<p><img src="http://7xqz39.com1.z0.glb.clouddn.com/tutorial-%E5%90%AF%E5%8A%A8%E8%B0%83%E8%AF%95%E5%91%BD%E4%BB%A4.png" alt=""></p>
<p><img src="http://7xqz39.com1.z0.glb.clouddn.com/final_result.png" alt=""> </p>
<ul>
<li>同时如果将窗口切换到远程访问界面的话，也会看到对应输出的日志信息(直接截图)</li>
</ul>
<p><img src="http://7xqz39.com1.z0.glb.clouddn.com/final_ssh.png" alt=""></p>
<ul>
<li>step 3 继续调试，直到程序结束，最终结果既不会显示在 IDE 的控制台输出信息中，而是会显示在 linux 的命令提示行中</li>
</ul>
<p><img src="http://7xqz39.com1.z0.glb.clouddn.com/end_ide.png" alt=""><br><img src="http://7xqz39.com1.z0.glb.clouddn.com/count.png" alt=""> </p>
<h2 id="u5173_u4E8E_u7ED3_u675F_u6536_u5C3E_u5DE5_u4F5C"><a href="#u5173_u4E8E_u7ED3_u675F_u6536_u5C3E_u5DE5_u4F5C" class="headerlink" title="关于结束收尾工作"></a>关于结束收尾工作</h2><h3 id="u9996_u5148_u7ED3_u675F_spark"><a href="#u9996_u5148_u7ED3_u675F_spark" class="headerlink" title="首先结束 spark"></a>首先结束 spark</h3><ul>
<li>将路径切换到 ${SPARK_HOME}/sbin</li>
<li>然后运行如下命令停止 spark 相关进程</li>
</ul>
<pre><code>
root@aimer-v:/spark/sbin# ./stop-all.sh 
localhost: stopping org.apache.spark.deploy.worker.Worker
stopping org.apache.spark.deploy.master.Master
</code></pre>

<h3 id="u7136_u540E_u7ED3_u675F_hadoop"><a href="#u7136_u540E_u7ED3_u675F_hadoop" class="headerlink" title="然后结束 hadoop"></a>然后结束 hadoop</h3><ul>
<li>将路径切换到 ${HADOOP_HOME}/sbin 下面</li>
<li>然后运行如下的命令来停止 hadoop 相关的进程</li>
</ul>
<pre><code>
root@aimer-v:/hadoop/sbin# ./stop-all.sh 
This script is Deprecated. Instead use stop-dfs.sh and stop-yarn.sh
Stopping namenodes on [localhost]
localhost: stopping namenode
localhost: stopping datanode
Stopping secondary namenodes [0.0.0.0]
0.0.0.0: stopping secondarynamenode
stopping yarn daemons
stopping resourcemanager
localhost: stopping nodemanager
no proxyserver to stop
</code></pre>

<h3 id="u6700_u540E_u8F93_u5165_jps__u547D_u4EE4_u6765_u68C0_u67E5_u662F_u5426_u76F8_u5173_u8FDB_u7A0B_u5747_u505C_u6B62"><a href="#u6700_u540E_u8F93_u5165_jps__u547D_u4EE4_u6765_u68C0_u67E5_u662F_u5426_u76F8_u5173_u8FDB_u7A0B_u5747_u505C_u6B62" class="headerlink" title="最后输入 jps 命令来检查是否相关进程均停止"></a>最后输入 jps 命令来检查是否相关进程均停止</h3><pre><code>
root@aimer-v:/spark/bin# jps
7305 SparkSubmit
10003 Jps
</code></pre>

<p>是的，本篇博客最大的败笔除了截图截得参差不齐之外，就是最后这个运行的 SparkSubmit 进程我不知道如何停止它；<br>直接 kill 没有生效，就当做是挖个坑好了，解决之后再来写上 （= . =）||</p>
<p>end</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://kylin27.github.io/2016/02/26/2016.2.26.remote_debug_spark_intellij/" data-id="cill04bb7001nrkimf547q7s1" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/IDEA/">IDEA</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Intellij/">Intellij</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/debug/">debug</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/java/">java</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/spark/">spark</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-2016.2.25.spark.manual.translate1" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2016/02/25/2016.2.25.spark.manual.translate1/" class="article-date">
  <time datetime="2016-02-24T16:00:00.000Z" itemprop="datePublished">2016-02-25</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2016/02/25/2016.2.25.spark.manual.translate1/">Spark 文档翻译1</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>Description: 最近参加了一个<a href="http://ifeve.com/apache-spark/comment-page-1/#comment-26703" target="_blank" rel="external">翻译 spark 文档的小组</a> 打算利用业余时间来补习一下英文，以及通过阅读 spark 文档来系统学习一下 spark 的相关知识，翻译难免有不恰当之处，敬请指正。</p>
<hr>
<p><a href="http://spark.apache.org/docs/latest/sql-programming-guide.html#spark-sql-dataframes-and-datasets-guide" target="_blank" rel="external">Spark SQL,DataFrames and Datasets Guide</a></p>
<h1 id="u6982_u8FF0"><a href="#u6982_u8FF0" class="headerlink" title="概述"></a>概述</h1><p>Spark SQL 是 Spark 中用来处理结构化数据的模块。基本抽象数据类型 RDD 所提供的接口不同的是， Spark SQL 的接口则会向 Spark 提供更多关于数据结构和正在进行的计算结构方面额外的信息。事实上， Spark SQL 就是利用这些附带的信息来执行额外的优化操作的。与 Spark SQL相交互的方式有这几种: 通过 SQL 语句，DataFrames 的接口函数和 Dataset 提供的应用程序接口函数。<br>计算数据的时所调用的是同一个引擎，该引擎不会因为你使用编程语言或是调用函数接口的不同而有所变动。这种(引擎调用的)统一化意味着开发者可以轻易地在 Spark 为不同语言提供的函数接口之间进行频繁地切换目的是可以用不同语言中最为地道的的方式来执行数据的转换操作。</p>
<p>在当前页面中所呈现的所有示例和简单的测试数据在 Spark 发布的软件包中均能找到，且可以使用 spark-shell,pyspark shell 或是 sparkR shell 来运行。</p>
<h3 id="SQL-_u7ED3_u6784_u5316_u67E5_u8BE2_u8BED_u8A00"><a href="#SQL-_u7ED3_u6784_u5316_u67E5_u8BE2_u8BED_u8A00" class="headerlink" title="SQL-结构化查询语言"></a>SQL-结构化查询语言</h3><p>Spark SQL 的其中一种使用方式便是用来执行执行最基本 SQL 语法格式或是 HiveQL 语法格式的 SQL 语句的查询。 Spark SQL 同样也具有从已经安装部署好的 Hive (数据仓库)中读取数据这样一种功能。如果想要了解更多关于如何配置 Spark SQL 的这种特性，可以参照 Hive Tables 这一段的文档。当使用另外一种编程语言来运行 SQL 的时候最终的执行结果将会以 DataFrame 的数据结构被返回使用者也可以通过命令行或是 JDBC/ODBC 的方式来与 SQL 接口来进行交互。</p>
<h3 id="DataFrames-_u6570_u636E_u6846"><a href="#DataFrames-_u6570_u636E_u6846" class="headerlink" title="DataFrames-数据框"></a>DataFrames-数据框</h3><p>DataFrame 所指的是由以’列’组织的这样的数据所构成的分布式集合。从理论角度分析，可以把存放于关系型数据库中的数据表比作是 R/Python 语言中的数据框，但是后者在底层表述方面有着更加丰富的优化策略。DataFrames 有着丰富的生成数据来源，像是: 结构化数据文件， Hive 中的数据表，外部数据库或是已经存在于内存中的 RDDs 结构。</p>
<p>(Spark 也为)DataFrame 提供了由 Scala，Java,Python，和R语言编写好的 API 接口。</p>
<h3 id="Datasets-_u6570_u636E_u96C6"><a href="#Datasets-_u6570_u636E_u96C6" class="headerlink" title="Datasets-数据集"></a>Datasets-数据集</h3><p>Dataset 是在 Spark 1.6 版本之后实验性新增的一个接口，目的是为了让基于 RDDs(支持强写入,并能够使用强大的 lambda 表达式) 可以在执行计算时具备和 Spark SQL 一样的引擎优化的能力。一个 Dataset(对象) 可以从 JVM 的对象生成，(生成之后)便可以调用相关的操作方法(像是,map, flatMap,filter,诸如此类的方法)。</p>
<p>Dataset 有一套统一的可被应用于 Scala 和 Java 开发语言中的API接口函数。Python 语言目前还不支持 Dataset 的 API 接口函数，但是由于 Python 本身具备动态编程语言的特性这一优势使使用 Dataset 的 API变为了可能(比如说，你可以通过 Python 的原声语言特sing row.columnName 来直接访问行对象中的属性字段)。Dataset 将会在未来的发行版本中来实现对 Python 语言的完全支持。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://kylin27.github.io/2016/02/25/2016.2.25.spark.manual.translate1/" data-id="cill04bbc001xrkimidyk6lb4" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/doc/">doc</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/official/">official</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/spark/">spark</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/翻译/">翻译</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-2016.2.19.Google-Test 使用教程" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2016/02/19/2016.2.19.Google-Test 使用教程/" class="article-date">
  <time datetime="2016-02-18T16:00:00.000Z" itemprop="datePublished">2016-02-19</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2016/02/19/2016.2.19.Google-Test 使用教程/">Google 测试框架 g-Test 使用教程</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>Description: 这篇文章简明介绍了 google 开源的一款 C++ 方向的测试框架 <a href="http://baike.baidu.com/link?url=BaRDfHesKkuurHcqjxM2g8vRMU9udYULmvOLn49k9pr79Il7uj6z7wlcf-BIe4sgJNLKTH8MN8MpMvdZseIEVK" target="_blank" rel="external">Gtest</a> 的使用方法做简单介绍,为后续整理发布的<a href="/">数据库引擎开发项目</a>(目前还没发布到博客中)做单元，模块测试方面的技术支撑。</p>
<hr>
<h2 id="G-Test__u7684_u5B89_u88C5"><a href="#G-Test__u7684_u5B89_u88C5" class="headerlink" title="G-Test 的安装"></a>G-Test 的安装</h2><ul>
<li>通过 github 页面下载 gtest 的源码压缩包,命令如下</li>
</ul>
<pre><code>
   $wget https://github.com/google/googletest/archive/master.zip 
</code></pre>

<ul>
<li>解压之后，在目录下面看到 CMakeList.txt 文件，可知使用的是 cmake 编译工具，保证虚拟机处于联网(NAT模式)，输入命令下载 cmake 编译工具</li>
</ul>
<pre><code>
  $apt-get install cmake 
</code></pre>

<ul>
<li>下载 cmake 结束之后，测试其是否能够正常工作 </li>
</ul>
<pre><code>
$cmake
如果正常工作会显示如下内容
Usage
  cmake [options] <path-to-source>
  cmake [options] <path-to-existing-build>
  Options -C <initial-cache> = Pre-load a script to populate the cache. 
</initial-cache></path-to-existing-build></path-to-source></code></pre>

<p> 编译安装 gTest </p>
<pre><code>
    $ mkdir cmake_dir
    $ cd cmake_dir
    $ cmake ../
    正确编译，显示信息如下
-- The C compiler identification is GNU 4.9.2
-- The CXX compiler identification is GNU 4.9.2
-- Check for working C compiler: /usr/bin/cc
-- Check for working C compiler: /usr/bin/cc -- works
...
-- Found Threads: TRUE  
-- Configuring done
-- Generating done
-- Build files have been written to: /gTester/googletest-master/cmake_dir
 </code></pre> 
   在这里我们创建 cmake_dir 文件夹，并在该文件夹的下面执行 cmake 操作的目的是为了让 cmake 编译生成的中间文件与下载的 gTest 的项目文件二者之间分离开，不然会混淆. 在 cmake_dir 文件夹中生成的中间文件如下:

<pre><code>
 CMakeCache.txt  CMakeFiles  cmake_install.cmake  CTestTestfile.cmake  googlemock  Makefile
</code></pre>

<p>   继续输入 make &amp;&amp; make install 来安装 gTest，如正确编译安装显示信息如下:</p>
<pre><code>
 Scanning dependencies of target gmock
[ 14%] Building CXX object googlemock/CMakeFiles/gmock.dir/__/googletest/src/gtest-all.cc.o
[ 28%] Building CXX object googlemock/CMakeFiles/gmock.dir/src/gmock-all.cc.o
Linking CXX static library libgmock.a
...
Install the project...
-- Install configuration: ""
-- Installing: /usr/local/lib/libgmock.a
-- Installing: /usr/local/lib/libgmock_main.a
-- Installing: /usr/local/include/gmock
-- Installing: /usr/local/include/gmock/gmock-generated-actions.h
-- Installing: /usr/local/include/gmock/gmock-cardinalities.h
-- Installing: /usr/local/include/gmock/gmock-actions.h
... 
</code></pre> 

<h2 id="gTest__u7684_u5165_u95E8_u7EA7_u4F7F_u7528_u4F8B_u5B50"><a href="#gTest__u7684_u5165_u95E8_u7EA7_u4F7F_u7528_u4F8B_u5B50" class="headerlink" title="gTest 的入门级使用例子"></a>gTest 的入门级使用例子</h2><ul>
<li>使用 gTest 来编写最简单的测试用例<ul>
<li>首先编写 C++ 文件  </li>
</ul>
</li>
</ul>
<pre><code>
  // test.hpp
#ifndef TEST_HPP__
#define TEST_HPP__

int getValue(int _value);

#endif
</code></pre>

<pre><code>
 // test.cpp
#include "test.hpp"
#include <cstdio>

int getValue( int _value ){
    return _value ;
}
</cstdio></code> </pre>


<ul>
<li>编写包含 gTest库函数的 C++ 测试文件 </li>
</ul>
<pre><code>
#include "test.hpp"
#include "gtest/gtest.h"

// first we test ASSERT_TRUE

void test_ASSERT_TRUE(){
  ASSERT_TRUE(false) ;
}

void test_EXPECT_TRUE(){
   EXPECT_TRUE(false) ;
}

int main( void ){
  test_EXPECT_TRUE();

  return 0;
}

</code></pre>

<ul>
<li>编写 Makefile 文件</li>
</ul>
<pre><code>
GTEST_DIR=/gTester/googletest-master/googletest

USER_DIR= ./

CPPFLAGS += -I$(GTEST_DIR)/include

CXXFLAGS += -g -Wall -Wextra

TESTS = test_tester

GTEST_HEADERS = $(GTEST_DIR)/include/gtest/*.h \
                $(GTEST_DIR)/include/gtest/internal/*.h

all : $(TESTS)

clean :
        rm -f $(TESTS) *.a *.o

GTEST_SRCS_= $(GTEST_DIR)/src/*.cc $(GTEST_DIR)/src/*.h $(GTEST_HEADERS)

gtest-all.o : $(GTEST_SRCS_)
        $(CXX) $(CPPFLAGS) -I$(GTEST_DIR) $(CXXFLAGS) -c \
                $(GTEST_DIR)/src/gtest-all.cc


gtest_main.o : $(GTEST_SRCS_)
        $(CXX) $(CPPFLAGS) -I$(GTEST_DIR) $(CXXFLAGS) -c $(GTEST_DIR)/src/gtest_main.cc  

gtest.a : gtest-all.o
        $(AR) $(ARFLAGS) $@ $^  

gtest_main.a : gtest-all.o gtest_main.o
        $(AR) $(ARFLAGS) $@ $^  


test.o : $(USER_DIR)/test.cpp $(USER_DIR)/test.hpp $(GTEST_HEADERS)
        $(CXX) $(CPPFLAGS) $(CXXFLAGS) -c $(USER_DIR)/test.cpp

test_tester.o : $(USER_DIR)/test_tester.cpp $(USER_DIR)/test.hpp $(GTEST_HEADERS)
        $(CXX) $(CPPFLAGS) $(CXXFLAGS) -c $(USER_DIR)/test_tester.cpp

test_tester : test.o test_tester.o gtest_main.a
        $(CXX) $(CPPFLAGS) $(CXXFLAG) -pthread  $^ -o $@
</code></pre>

<ul>
<li>输入 make 命令生成可执行文件 test_tester</li>
<li>运行 gTest 的测试文件，查看输出结果</li>
</ul>
<pre><code>
 .//test_tester.cpp:12: Failure
  Value of: false
  Actual: false
  Expected: true 
</code> </pre>

<h2 id="gTest__u4E2D_u65AD_u8A00_u4ECB_u7ECD"><a href="#gTest__u4E2D_u65AD_u8A00_u4ECB_u7ECD" class="headerlink" title="gTest 中断言介绍"></a>gTest 中断言介绍</h2><ul>
<li><p><b>ASSERT_* 的断言函数如果判定最终结果不满足判定输出值，将会发出 断言失败 + 终止程序的结果</b></p>
</li>
<li><p><b>EXPECT_* 的断言函数如果判断最终结果不满足判定输出值，将仅会发出 断言失败 的提示信息</b>    </p>
</li>
</ul>
<h3 id="u57FA_u672C_u65AD_u8A00_u8BF4_u660E"><a href="#u57FA_u672C_u65AD_u8A00_u8BF4_u660E" class="headerlink" title="基本断言说明"></a>基本断言说明</h3><pre><code>
ASSERT_TRUE （condition）； 
ASSERT_FALSE (condition) ；
</code></pre>
上述断言函数是这样的: 括号中的 condition 可以是一个返回结果为布尔值的函数，也可以是一个不二变量，同样也可以是一个逻辑表达式，只要最终返回的结果是布尔值就可以。而两个函数 ASSERT_TRUE 要求这个布尔值必须是 TRUE/真值，如果不是真便会输出'致命错误'并退出当前正在执行的函数。 ASSERT_FALSE 方法刚好相反，它期待的是一个 FALSE/假值，如果不满足同样输出'致命错误'信息，并退出当前正在执行的方法(程序)。

<pre><code>
   EXPECT_TRUE(condition) ;
   EXPECT_FALSE(condition) ;
</code></pre>
上述函数的断言是这样的: 如果括号中的 condition 变量返回的布尔值与断言期待的布尔值不同的话，不会退出当前正在执行的方法/程序。 它会继续允许程序的继续运行，但是会输出执行错误的提示信息（如果错误信息流定向是控制台显示器的话）

### 基于两值比较的断言函数说明
+ 在基于而知比较的断言函数中，传入的参数必须要满足下面两种条件中的一种
  + 是基本类型，可以直接进行逻辑比较

  + 如果是符合类似(class ,struct)是需要重载比较运算符的

  + 需要注意的是，如果传入的是指针类型的话，判定的并不是指针指向的数值内容是否相同，而是会判定指针是否指向同一块内存空间
  + <b>如果需要判定指针指向字符串的逻辑关系，不要使用这一系列的断言函数</b> <br>


<pre><code>
    ASSERT_EQ(expected, actual) ；  expected==actual
    ASSERT_NE(expected, actual) ；  expected!=actual
    ASSERT_LT(expected, actual) ；  expected 小于 actual, LT(less than)
    ASSERT_LE(expected, actual) ；  expected 小于等于 actual, LE(less equal)
    ASSERT_GT(expected, actual) ；  expected 大于 actual, GT(greater than)
    ASSERT_GE(expected, actual) ；  expected 大于等于 actual, GE(greater equal)
</code></pre>

<pre><code>
  EXPECT_EQ(expected, actual) ；
  EXPECT_NE(expected, actual) ；
  EXPECT_LT(expected, actual) ；
  EXPECT_LE(expected, actual) ；
  EXPECT_GT(expected, actual) ；
  EXPECT_GE(expected, actual) ；
</code></pre>



<h3 id="u57FA_u4E8E_u5B57_u7B26_u4E32_u7684_u6BD4_u8F83_u65AD_u8A00_u8BF4_u660E"><a href="#u57FA_u4E8E_u5B57_u7B26_u4E32_u7684_u6BD4_u8F83_u65AD_u8A00_u8BF4_u660E" class="headerlink" title="基于字符串的比较断言说明"></a>基于字符串的比较断言说明</h3><pre><code>
  ASSERT_STREQ(str1,str2); str1's content = str2's content
  ASSERT_STRNE(str1,str2); str1's content != str2's content
 ASSERT_STRCASEEQ(str1,str2); str1's content = str2's content ,ignoring characters case
 ASSERT_STRCASENE(str1,str2); ignoring string characters' case, str1's content != str2's content 
</code></pre>



      
    </div>
    <footer class="article-footer">
      <a data-url="http://kylin27.github.io/2016/02/19/2016.2.19.Google-Test 使用教程/" data-id="cill04bbf0024rkimdofut9fo" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/C/">C++</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Gtest/">Gtest</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/gtest/">gtest</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/manual/">manual</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/tutorial/">tutorial</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/测试/">测试</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-2016.2.18.dht.note" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2016/02/18/2016.2.18.dht.note/" class="article-date">
  <time datetime="2016-02-17T16:00:00.000Z" itemprop="datePublished">2016-02-18</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2016/02/18/2016.2.18.dht.note/">DHT Chord 理论学习笔记 1</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>Description:</p>
<p>本篇博客记录的是P2P 对等计算中基于 DHT 拓扑结构中的 Chord 算法理论信息，目的是为<a href="/">基于Chord算法的 P2P 自感应系统</a>代码做理论支撑</p>
<hr>
<h1 id="u540D_u8BCD_u6982_u5FF5"><a href="#u540D_u8BCD_u6982_u5FF5" class="headerlink" title="名词概念"></a>名词概念</h1><h2 id="DHT__u662F_u4EC0_u4E48__3F"><a href="#DHT__u662F_u4EC0_u4E48__3F" class="headerlink" title="DHT 是什么 ?"></a>DHT 是什么 ?</h2><p>DHT 是对等计算(Peer-to-Peer,P2P) 技术中拓扑结构中的一种，叫做<b>全分布式结构化结构</b>(decentralized structured topology),简称为 DHT，又称作<b>分布式哈希表</b>(distributed hash table)。<br></p>
<p>另外几种拓扑结构分别是<b>中心化拓扑结构</b>(centralized topology),<b>全分布式非结构化结构</b>(decentralized unstructured topology),<b>半分布式结构</b>(partially decentralized topology)。</p>
<h2 id="DHT__u8F83_u6BD4_u5176_u4ED6_u62D3_u6251_u7ED3_u6784_u6709_u54EA_u4E9B_u7279_u70B9_3F"><a href="#DHT__u8F83_u6BD4_u5176_u4ED6_u62D3_u6251_u7ED3_u6784_u6709_u54EA_u4E9B_u7279_u70B9_3F" class="headerlink" title="DHT 较比其他拓扑结构有哪些特点?"></a>DHT 较比其他拓扑结构有哪些特点?</h2><p>DHT 结构可以自适应的支持网络中节点动态地加入/退出，并且扩展性、鲁棒性、结点ID 分配的均匀性和自组织能力都很好。同时DHT由于自身结构的特点可以提供精确的发现和定位网络中结点资源的功能。</p>
<h2 id="DHT__u90FD_u53EF_u4EE5_u7528_u6765_u505A_u4EC0_u4E48__3F"><a href="#DHT__u90FD_u53EF_u4EE5_u7528_u6765_u505A_u4EC0_u4E48__3F" class="headerlink" title="DHT 都可以用来做什么 ?"></a>DHT 都可以用来做什么 ?</h2><p>可以基于 DHT 来建立复杂的服务，例如分散式档案系统、点对点技术档案分享系统、网页的快速抓取、数据的缓存系统、网络中任意结点数据传输、网域名城系统和即时通讯系统等等。 </p>
<h2 id="DHT__u7531_u54EA_u4E9B_u5143_u7D20_u6784_u6210"><a href="#DHT__u7531_u54EA_u4E9B_u5143_u7D20_u6784_u6210" class="headerlink" title="DHT 由哪些元素构成"></a>DHT 由哪些元素构成</h2><h3 id="u64CD_u4F5C/_u65B9_u6CD5_3A_Put_28key_2Cdata_29_2C_Get_28key_29"><a href="#u64CD_u4F5C/_u65B9_u6CD5_3A_Put_28key_2Cdata_29_2C_Get_28key_29" class="headerlink" title="操作/方法: Put(key,data), Get(key)"></a>操作/方法: Put(key,data), Get(key)</h3><p> 既然你已经知道了 DHT 就是我们常说的分布式哈希表，那么结合哈希表的’存入’,’读出’的操作特性，便可以推知 DHT 的操作方法也不外乎这两个(put,get)。</p>
<p> 更加详细的说明在介绍 Chord 算法之后再来补充。</p>
<h2 id="Chord__u662F_u4EC0_u4E48__3F"><a href="#Chord__u662F_u4EC0_u4E48__3F" class="headerlink" title="Chord 是什么 ?"></a>Chord 是什么 ?</h2><p> Chord 是基于 DHT 数据结构的分布式算法，如果说 DHT 为分布式系统查询提供 Put , Get 接口封装的话，那么 Chord 便为 DHT 中为 Get,Put 两种两种方法提供底层实现的算法。 </p>
<p> 二者的关系如下图所示</p>
<p> <img src="http://7xqz39.com1.z0.glb.clouddn.com/2016.2.17.dht.1dht.1.png" alt=""></p>
<ul>
<li><p>在 Chord 算法将每台主机抽象成结点(Node)，同时为了保证结点在网络中的唯一性， 使用 Node-ID 数字序列来作为结点的唯一标识。</p>
</li>
<li><p>Chord 将网络中处于’在线’状态的计算机(结点)，按照<b>Node-ID</b>数值的大小排列，从逻辑上构成一个首位相连的环形结构。</p>
</li>
<li><p>为了方便在网络中每台’在线’结点上资源的搜索，每个结点上面还存放用来存放其他结点信息的<b>路由表</b>。</p>
</li>
<li><p>其中逻辑环中的主机总数(结点数目)和结点 ID 号码的位数都有关系:</p>
</li>
</ul>
<figure class="highlight crmsh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">Node</span><span class="identifier"></span><span class="title">-ID</span> 二进制位数 = m</span><br><span class="line"></span><br><span class="line">逻辑环中最大容纳主机(结点)个数 = <span class="number">2</span>^m 个</span><br><span class="line"></span><br><span class="line"><span class="keyword">Node</span><span class="identifier"></span><span class="title">-ID</span> 取值范围 = [<span class="number">0</span>,<span class="number">2</span>^m -<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">Node</span><span class="identifier"> </span><span class="title">上面的 finger-table</span>(路由表) 维护的表项个数为 m 个</span><br><span class="line"></span><br><span class="line">Finger-table 的表项中记录其他 <span class="keyword">Node</span><span class="identifier"></span><span class="title">-ID</span> 的ID间隔为 <span class="number">2</span>^i (<span class="number">0</span> <span class="tag">&lt;=i &lt;= m-1)</span></span><br></pre></td></tr></table></figure>
<p> P2P 网络中 Chord 环状结构图示：下图便是一个 m 为 6 的Chord 拓扑环和环中Node-ID = 8 结点上所维护的 finger table 图示</p>
<p> <img src="http://7xqz39.com1.z0.glb.clouddn.com/2016.2.17.dht.1dht.2.png" alt=""></p>
<h2 id="Chord__u6784_u6210_u6570_u636E_u7ED3_u6784_u4ECB_u7ECD"><a href="#Chord__u6784_u6210_u6570_u636E_u7ED3_u6784_u4ECB_u7ECD" class="headerlink" title="Chord 构成数据结构介绍"></a>Chord 构成数据结构介绍</h2><h3 id="u540E_u7EE7_u7ED3_u70B9_u5217_u8868_-_sucessor_node_list"><a href="#u540E_u7EE7_u7ED3_u70B9_u5217_u8868_-_sucessor_node_list" class="headerlink" title="后继结点列表 - sucessor node list"></a>后继结点列表 - sucessor node list</h3><ul>
<li><p>何谓后继结点? </p>
<ul>
<li>后继结点(successor)指的是 Chord 逻辑环中处于在线状态的 Node-ID 数值 &gt;= 当前结点 Node-ID 的结点所构成的结点集合中 Node-ID 数值最小的结点变叫做当前结点的后继结点-它只有一个。列表中的其他的节点都是 Node-ID &gt; 当前结点的 Node-ID。</li>
</ul>
</li>
</ul>
<ul>
<li>这么说有点混乱,举个例子: 就拿上图而言，我们设 N14 为当前节点，那么对于 N14 来说整个 Chord 环中 Node-ID 大于等于它所构成的结点集合是 {N21,N32,N38,N42,N48,N51,N56}。</li>
</ul>
<ul>
<li>{N21,N32,N38,N42,N48,N51,N56}  这个列表便是当前 N14的后继结点列表 ，而其中 Node-ID 号码最小的是 N21 , 那么 N21 便可以称作是 N14 的后继结点。</li>
</ul>
<ul>
<li><b>Chord 逻辑环中所提及的结点必须是处于’上线’/live 状态的节点。 Chord 中将处于在线状态的结点作为环中结点进行添加，如果不处于在线状态是不会将它加入逻辑环结构中的。 如果一个结点存活着进入到环中，然后由于某种原因崩溃/宕机了，那么它会被从环中踢出去的。</b> </li>
</ul>
<ul>
<li>后继结点列表是构成 Chord 网络的主要数据结构，当前结点可以借助于后继结点列表中记录的信息来直接’跳转’到它的后继结点上面，就像是 C++ 中通过指针进行地址跳转一样。</li>
</ul>
<ul>
<li><p>后继结点列表越大越好吗？</p>
<ul>
<li><p>后接结点列表中存放的表项越多整个环形网络搜索到目标资源的可靠性越高，设想如果通过当前结点可以’跳到’更多的结点的上面，想必在环形网络中搜索到目标资源的命中率越高。</p>
</li>
<li><p>不过，这些是相对于网络数据流量而言的，维护的列表越大，网络负担也就越重。从一个结点发出的查询越多(可跳转到的结点越多)，该结点的流量相比也会越大。</p>
</li>
</ul>
</li>
</ul>
<h3 id="u524D_u9A71_u7ED3_u70B9_-_predecessor_node"><a href="#u524D_u9A71_u7ED3_u70B9_-_predecessor_node" class="headerlink" title="前驱结点 - predecessor node"></a>前驱结点 - predecessor node</h3><ul>
<li><p>何谓前驱结点?</p>
<ul>
<li>在了解何为后继结点之后，前驱结点的概念也明朗了很多。它指的是 Node-ID &lt; 当前结点 Node-ID 的所有结点集合中， Node-ID 最小的那个便是当前结点的前驱结点</li>
</ul>
</li>
</ul>
<ul>
<li><p>如果某个结点没有 Node-ID &lt; 它的 Node-ID 的结点的话，那么便选取整个 Chord 网络中 Node-ID 号码最大的结点作为它的前驱结点。N8 便是例子，它的前驱结点是 N56。 </p>
</li>
<li><p>这种特例用在后继结点上也是一样:如果在 Chord 网络中找不到 Node-ID &gt; 它的 Node-ID 结点的话，就选取 Chord 网络中 Node-ID 最小的 Node-ID 所标识的结点作为它的后继结点。例如 N56 结点的后继结点便是 N8.</p>
</li>
</ul>
<h3 id="u8DEF_u7531_u8868_-_finger_table"><a href="#u8DEF_u7531_u8868_-_finger_table" class="headerlink" title="路由表 - finger table"></a>路由表 - finger table</h3><ul>
<li><p>何谓路由表? </p>
<p>路由表主要用来提高结点和资源信息的查询路由的速度，类似于 linux 中用来记录 IP 和 IP 所映射的域名的路由表，因此得名。 就是为了根据当前结点可以快速的跳转到其他结点上而记录的其他结点的{结点名称:网络地址} 这样映射关系的表项的二维表格。</p>
</li>
</ul>
<h4 id="u8DEF_u7531_u8868_u8868_u9879"><a href="#u8DEF_u7531_u8868_u8868_u9879" class="headerlink" title="路由表表项"></a>路由表表项</h4><p>   路由表中表项主要存放了一下几种信息:</p>
<ul>
<li><p>start :    (n+2^(k-1)) mode 2^m ; (1&lt;= k &lt;= m )起始查询结点</p>
</li>
<li><p>interval :    [finger[i].start,finger[i+1].start) 区间范围</p>
</li>
<li><p>node : Chord 网络中第一个 Node-ID &gt;= start 结点的后继结点</p>
</li>
<li><p>successor : Chord 网络中结点的直接后继结点</p>
</li>
<li><p>predecessor : Chord 环形网络中结点的直接前驱结点</p>
</li>
</ul>
<p>end</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://kylin27.github.io/2016/02/18/2016.2.18.dht.note/" data-id="cill04bbt002hrkimg3gpk9mo" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Chord/">Chord</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/DHT/">DHT</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/P2P/">P2P</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/note/">note</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/theory/">theory</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-2016.2.17.upload_picture" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2016/02/17/2016.2.17.upload_picture/" class="article-date">
  <time datetime="2016-02-16T16:00:00.000Z" itemprop="datePublished">2016-02-17</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2016/02/17/2016.2.17.upload_picture/">如何使用七牛存储来为你的 gihub 博客中添加图片</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>Description:<br>作为一个截图狂魔的我，在写博客的时候为了让内容描述更加的简洁，都会上传大量的图片实例。<br>但是自从换成了 github 上面的博客之后，为了节省空间(主要是技术问题…)，一直都没有上传图片(背景图片除外！)<br>近期写的<a href="https://kylin27.github.io/2016/02/18/2016.2.18.dht.note/">DHT 这篇博客</a>十分需要配图说明，所以在这篇博客中将如何使用七牛存储来为自己的博客中添加图片的步骤记录一下。</p>
<hr>
<h1 id="u9996_u5148_u6CE8_u518C_u7533_u8BF7_u4E03_u725B_u7684_u8D26_u53F7"><a href="#u9996_u5148_u6CE8_u518C_u7533_u8BF7_u4E03_u725B_u7684_u8D26_u53F7" class="headerlink" title="首先注册申请七牛的账号"></a>首先注册申请<a href="https://portal.qiniu.com/signin" target="_blank" rel="external">七牛</a>的账号</h1><ul>
<li>在注册的时候，推荐将自己的 github 账号和七牛二者进行绑定。</li>
<li>注册成功并登录之后，需要配置空间，也就是为你的空间起域名，空间选择’公开’这样可以确保通过 github 上的链接信息可以直接访问</li>
<li>点击内容管理，然后从本地上传一张图片，会在右边窗口栏中看到这张图片的外链地址信息，我的是<a href="http://7xqz39.com1.z0.glb.clouddn.com/tutorial-test.png" target="_blank" rel="external">这个</a> 点击它便可以看到刚刚上传的截图信息了</li>
</ul>
<h1 id="u7136_u540E_u5C06_u5916_u94FE_u5730_u5740_u6DFB_u52A0_u5230_u535A_u5BA2_u4E2D"><a href="#u7136_u540E_u5C06_u5916_u94FE_u5730_u5740_u6DFB_u52A0_u5230_u535A_u5BA2_u4E2D" class="headerlink" title="然后将外链地址添加到博客中"></a>然后将外链地址添加到博客中</h1><p> <code>加载图片的格式为 ![](刚才七牛网站空间上传图片所生成的外链地址)</code></p>
<p> <img src="http://7xqz39.com1.z0.glb.clouddn.com/2016.2.17.dht.1hehe.png" alt="">  </p>
<p> 这样便可将图片展现在博客中(你没有穿越，前几天手贱把七牛里面的图片都删了，然后图片链接就坏掉了，这是重新截的图)</p>
<p> end</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://kylin27.github.io/2016/02/17/2016.2.17.upload_picture/" data-id="cill04bbz002rrkimheojcb6o" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/blog/">blog</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/github/">github</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/hexo/">hexo</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/picture/">picture</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/七牛/">七牛</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-2016.2.14.docker_spark" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2016/02/14/2016.2.14.docker_spark/" class="article-date">
  <time datetime="2016-02-13T16:00:00.000Z" itemprop="datePublished">2016-02-14</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2016/02/14/2016.2.14.docker_spark/">使用 docker 来构建 spark 集群</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>Description:<br>在<a href="https://kylin27.github.io/2016/02/13/2016.2.13.docker_install/">上一博客</a>中介绍了如何在 ubuntu 上面部署 docker 系统，以及 docker 的基本命令; 在本篇博客中将会介绍如何利用 docker-hub 上的资源来快速搭建一个 spark 集群</p>
<hr>
<h1 id="u5982_u4F55_u4F7F_u7528_dockerfile__u6765_u751F_u6210_docker__u955C_u50CF_u6587_u4EF6"><a href="#u5982_u4F55_u4F7F_u7528_dockerfile__u6765_u751F_u6210_docker__u955C_u50CF_u6587_u4EF6" class="headerlink" title="如何使用 dockerfile 来生成 docker 镜像文件"></a>如何使用 dockerfile 来生成 docker 镜像文件</h1><p> 在后续的步骤中有一个镜像文件是无法直接从 docker-hub 的镜像库中直接下载的，但是可以从 github 上面下载到该镜像文件的 dockerfile 文件。 所以在这里介绍一下如何使用 dockerfile 来创建 docker 镜像文件。 如果今后需要的话，将会详细介绍一下如何根据自己的需要来定制编写自己的 dockerfile .</p>
<h3 id="docker_build"><a href="#docker_build" class="headerlink" title="docker build"></a>docker build</h3><p> 用来将指定路径的中 dockerfile 生成 docker 镜像文件</p>
 <pre>
 docker build -t="amplab/apache-hadoop-hdfs-precise:1.2.1" . 

 // 上述命令会搜索当前路径，看是否有 dockerfile 文件，如果有，那么执行该 dockerfile 文件，并根据该 dockerfile 文件生成
 // docker 镜像文件。同时又将该镜像文件打上名为 'amplab/apache-hadoop-hdfs-precise:1.2.1' 的标签
 // 由于后面会通过 github 上面提供的 deploy.sh 脚本来构建 spark 系统，所以尽最大可能的保持 hadoop 的镜像文件的名称一致性 
 </pre>


<h1 id="u4ECE_docker-hub__u4E2D_u4E0B_u8F7D_u955C_u50CF_u6587_u4EF6"><a href="#u4ECE_docker-hub__u4E2D_u4E0B_u8F7D_u955C_u50CF_u6587_u4EF6" class="headerlink" title="从 docker-hub 中下载镜像文件"></a>从 docker-hub 中下载镜像文件</h1><p>首先确保正确登录 docker-hub 账号<br>其实我安装 spark 的主要目的是为了使用 spark 提供的 GraphX 和 mllib 这两个工具，而 mllib 中在 spark-0.9 之后才支持，<br>所以，在这里我安装的是 spark-1.0.0 版本<br><b>在执行 pull 命令之前,我开启了翻墙的软件,这样可以节省时间</b></p>
<pre>
sudo docker pull amplab/apache-hadoop-hdfs-precise:1.2.1   
// 这个镜像在 docker-hub 上面找不到，所以需要根据 github 可以获得的 Dockerfile 来构建其镜像文件

$docker pull amplab/dnsmasq-precise:1.0.0
$docker pull amplab/spark-worker:1.0.0
$docker pull amplab/spark-master:1.0.1
$docker pull amplab/spark-shell:1.0.1

$docker images                // 通过该命令查看系统中的镜像文件

REPOSITORY               TAG                 IMAGE ID            CREATED             VIRTUAL SIZE
ubuntu_14                wget                44552cea1d79        7 hours ago         187.9 MB
ubuntu                   14.04               8693db7e8a00        3 weeks ago         187.9 MB
amplab/spark-shell       1.0.0               c18acb8d81a0        20 months ago       964.3 MB
amplab/spark-worker      1.0.0               6f77966546ee        20 months ago       964.3 MB
amplab/spark-master      1.0.0               a43b969cfeff        20 months ago       964.3 MB
amplab/dnsmasq-precise   latest              d9cdba2ae123        23 months ago       205.8 MB

</pre>

<h1 id="u4ECE_git-hub__u4E0A_u9762_u4E0B_u8F7D_u8FD0_u884C_docker__u955C_u50CF_u6587_u4EF6_u7684_u811A_u672C"><a href="#u4ECE_git-hub__u4E0A_u9762_u4E0B_u8F7D_u8FD0_u884C_docker__u955C_u50CF_u6587_u4EF6_u7684_u811A_u672C" class="headerlink" title="从 git-hub 上面下载运行 docker 镜像文件的脚本"></a>从 git-hub 上面下载运行 docker 镜像文件的脚本</h1><pre>
$wget https://github.com/amplab/docker-scripts/archive/master.zip
$unzip master.zip
</pre>

<h1 id="u542F_u52A8_spark__u96C6_u7FA4"><a href="#u542F_u52A8_spark__u96C6_u7FA4" class="headerlink" title="启动 spark 集群"></a>启动 spark 集群</h1><p>将路径切换到包含 /deploy 文件夹的路径下面</p>
<pre>
./deploy/deploy.sh -i amplab/spark:1.0.0 -w 3

$docker ps                 // 通过该命令来查看系统中正在运行的容器信息

CONTAINER ID        IMAGE                           COMMAND                CREATED              STATUS              PORTS                NAMES
855415af179d        amplab/spark-shell:1.0.0        "/root/spark_shell_f   About a minute ago   Up About a minute   8888/tcp             cocky_meitner           
5bb8f871e474        amplab/spark-shell:1.0.0        "/root/spark_shell_f   2 minutes ago        Up 2 minutes        8888/tcp             modest_brown            
2c2d49565559        amplab/spark-worker:1.0.0       "/root/spark_worker_   5 minutes ago        Up 5 minutes        8888/tcp             silly_ptolemy           
24b16b64e6ad        amplab/spark-worker:1.0.0       "/root/spark_worker_   5 minutes ago        Up 5 minutes        8888/tcp             compassionate_lalande   
25efa7bbeb20        amplab/spark-worker:1.0.0       "/root/spark_worker_   5 minutes ago        Up 5 minutes        8888/tcp             happy_curie             
1ba8d72a6cd2        amplab/spark-master:1.0.0       "/root/spark_master_   6 minutes ago        Up 6 minutes        7077/tcp, 8080/tcp   kickass_jones           
3a8c3a8906df        amplab/dnsmasq-precise:latest   "/root/dnsmasq_files   6 minutes ago        Up 6 minutes 

</pre>

<p>具体的参数解释可以戳<a href="https://github.com/amplab/docker-scripts#testing" target="_blank" rel="external">这里</a><br>-w 是用来指定 spark 运行之后对应的 worker 进程个数<br>从上面输入 docker ps 命令之后显示出来的信息可以推知，我们总共创建了: spark-shell , spark-worker , spark-master, dns<br>这些容器运行，但是没有 hadoop 运行, 于是在熟悉上述流程的我又查了其他的几个spark 集群镜像文件，<a href="https://hub.docker.com/r/sequenceiq/spark/" target="_blank" rel="external">找了一个合适的</a> 接下来试试这个镜像文件好了… </p>
<hr>
<h1 id="u4E0D_u8FC7_u5148_u6765_u5C06_u5F53_u524D_u7CFB_u7EDF_u4E2D_u7684_u6240_u6709_u8FD0_u884C_u5BB9_u5668_u505C_u6B62"><a href="#u4E0D_u8FC7_u5148_u6765_u5C06_u5F53_u524D_u7CFB_u7EDF_u4E2D_u7684_u6240_u6709_u8FD0_u884C_u5BB9_u5668_u505C_u6B62" class="headerlink" title="不过先来将当前系统中的所有运行容器停止"></a>不过先来将当前系统中的所有运行容器停止</h1><pre>
// 同样现将路径切换到 ./deploy 文件夹的路径下面
$ ./deploy/kill_all.sh spark
$ ./deploy/kill_all.sh namespace
</pre>

<hr>
<h1 id="u5C06_u8BE5_u955C_u50CF_u6587_u4EF6_u4E0B_u8F7D_u5230_u672C_u5730"><a href="#u5C06_u8BE5_u955C_u50CF_u6587_u4EF6_u4E0B_u8F7D_u5230_u672C_u5730" class="headerlink" title="将该镜像文件下载到本地"></a>将该镜像文件下载到本地</h1><pre>
$docker pull sequenceiq/spark:v1.6.0onHadoop2.6.0
成功下载显示信息:
...
95d969caad90: Download complete 
2d727ce74b86: Download complete 
28c9338da9a6: Download complete 
cb7d9861a895: Download complete 
73bb712333d9: Download complete 
a466bc76549f: Download complete 
441cf02fdf7d: Download complete 
056efae329d8: Download complete 
Status: Downloaded newer image for sequenceiq/spark:v1.6.0onHadoop2.6.0
</pre>

<h1 id="u4ECE_docker__u955C_u50CF_u6587_u4EF6_u751F_u6210_u5E76_u8FD0_u884C_u5BB9_u5668"><a href="#u4ECE_docker__u955C_u50CF_u6587_u4EF6_u751F_u6210_u5E76_u8FD0_u884C_u5BB9_u5668" class="headerlink" title="从 docker 镜像文件生成并运行容器"></a>从 docker 镜像文件生成并运行容器</h1><p>先来查看一下当前系统中所有的镜像文件</p>
<pre>
$docker images     // 先来查看一下当前系统中所有的镜像文件
REPOSITORY               TAG                   IMAGE ID            CREATED             VIRTUAL SIZE
ubuntu_14                wget                  44552cea1d79        24 hours ago        187.9 MB
ubuntu                   14.04                 8693db7e8a00        3 weeks ago         187.9 MB
sequenceiq/spark         v1.6.0onHadoop2.6.0   056efae329d8        5 weeks ago         2.877 GB
amplab/spark-shell       1.0.0                 c18acb8d81a0        20 months ago       964.3 MB
amplab/spark-worker      1.0.0                 6f77966546ee        20 months ago       964.3 MB
amplab/spark-master      1.0.0                 a43b969cfeff        20 months ago       964.3 MB
amplab/dnsmasq-precise   latest                d9cdba2ae123        23 months ago       205.8 MB
</pre>

<p>再运行镜像文件生成容器实例</p>
<pre>
$docker run -it sequenceiq/spark:v1.6.0onHadoop2.6.0  bash 

成功运行显示信息:
Starting sshd:                                             [  OK  ]
Starting namenodes on [1f25c1d3d790]
1f25c1d3d790: starting namenode, logging to /usr/local/hadoop/logs/hadoop-root-namenode-1f25c1d3d790.out
localhost: starting datanode, logging to /usr/local/hadoop/logs/hadoop-root-datanode-1f25c1d3d790.out
Starting secondary namenodes [0.0.0.0]
<b>上述 docker 运行命令作用是是从刚刚下载到本地的 docker 镜像文件中生成 docker 容器(该容器中就包含部署好了的 hadoop 和 spark 软件)；
生成容器之后，登录到该容器中，并运行 bash 命令</b>

成功登录显示信息:
bash-4.1# ls     // <b>先显示一下容器中的基本信息 </b>      
bin  boot  dev  etc  home  lib  lib64  media  mnt  opt  pam-1.1.1-17.el6.src.rpm  proc  root  rpmbuild  sbin  selinux  srv  sys  tmp  usr  var
bash-4.1# jps     // <b>然后查看一下容器系统中运行的进程都有什么。 可以看出有 Hadoop 节点和 Spark 等相关进程在运行</b> 
562 NodeManager
353 SecondaryNameNode
109 NameNode
183 DataNode
636 Jps
482 ResourceManager
</pre>

<h1 id="u8FD0_u884C_spark__u4E2D_u7684_counter__u6D4B_u8BD5_u7A0B_u5E8F"><a href="#u8FD0_u884C_spark__u4E2D_u7684_counter__u6D4B_u8BD5_u7A0B_u5E8F" class="headerlink" title="运行 spark 中的 counter 测试程序"></a>运行 spark 中的 counter 测试程序</h1><pre>
<b>首先需要运行一下 spark-shell , 直接在当前 bash 命令行中输入如下命令 </b>
$spark-shell \
 --master yarn-client \
 --driver-memory 1g \
 --executor-memory 1g \
 --executor-cores 1

<b>如果成功，将会显示如下信息:</b>

16/02/14 02:57:02 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
16/02/14 02:57:03 INFO spark.SecurityManager: Changing view acls to: root
16/02/14 02:57:03 INFO spark.SecurityManager: Changing modify acls to: root
16/02/14 02:57:03 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(root); users with modify permissions: Set(root)
16/02/14 02:57:04 INFO spark.HttpServer: Starting HTTP Server
16/02/14 02:57:04 INFO server.Server: jetty-8.y.z-SNAPSHOT
16/02/14 02:57:04 INFO server.AbstractConnector: Started SocketConnector@0.0.0.0:58832
16/02/14 02:57:04 INFO util.Utils: Successfully started service 'HTTP class server' on port 58832.
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /___/ .__/\_,_/_/ /_/\_\   version 1.6.0
      /_/

Using Scala version 2.10.5 (Java HotSpot(TM) 64-Bit Server VM, Java 1.7.0_51)
Type in expressions to have them evaluated.
... 后面反正还有挺多，最后会看到 scala> 命令行提示输入符

<b>我们来测试一个最简单的计数程序调用好了</b>

scala> sc.parallelize( 1 to 1000).count()
16/02/14 03:01:56 INFO spark.SparkContext: Starting job: count at <console>:28
16/02/14 03:01:56 INFO scheduler.DAGScheduler: Got job 0 (count at <console>:28) with 2 output partitions
16/02/14 03:01:56 INFO scheduler.DAGScheduler: Final stage: ResultStage 0 (count at <console>:28)
16/02/14 03:01:56 INFO scheduler.DAGScheduler: Parents of final stage: List()
16/02/14 03:01:56 INFO scheduler.DAGScheduler: Missing parents: List()
16/02/14 03:01:56 INFO scheduler.DAGScheduler: Submitting ResultStage 0 (ParallelCollectionRDD[0] at parallelize at <console>:28), which has no missing parents
16/02/14 03:01:57 INFO storage.MemoryStore: Block broadcast_0 stored as values in memory (estimated size 1096.0 B, free 1096.0 B)
16/02/14 03:01:57 INFO storage.MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 804.0 B, free 1900.0 B)
16/02/14 03:01:57 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.17.0.10:48410 (size: 804.0 B, free: 517.4 MB)
16/02/14 03:01:57 INFO spark.SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1006
16/02/14 03:01:57 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ResultStage 0 (ParallelCollectionRDD[0] at parallelize at <console>:28)
16/02/14 03:01:57 INFO cluster.YarnScheduler: Adding task set 0.0 with 2 tasks
16/02/14 03:01:58 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, 42ba3f37ce84, partition 0,PROCESS_LOCAL, 2078 bytes)
16/02/14 03:01:58 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, 42ba3f37ce84, partition 1,PROCESS_LOCAL, 2135 bytes)
16/02/14 03:02:04 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on 42ba3f37ce84:58577 (size: 804.0 B, free: 517.4 MB)
16/02/14 03:02:04 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on 42ba3f37ce84:36276 (size: 804.0 B, free: 517.4 MB)
16/02/14 03:02:09 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 11580 ms on 42ba3f37ce84 (1/2)
16/02/14 03:02:09 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 11454 ms on 42ba3f37ce84 (2/2)
16/02/14 03:02:09 INFO cluster.YarnScheduler: Removed TaskSet 0.0, whose tasks have all completed, from pool 
16/02/14 03:02:09 INFO scheduler.DAGScheduler: ResultStage 0 (count at <console>:28) finished in 11.657 s
16/02/14 03:02:09 INFO scheduler.DAGScheduler: Job 0 finished: count at <console>:28, took 13.058068 s
res0: Long = 1000
</console></console></console></console></console></console></console></pre>

<p> 这样一个 spark 集群就搭建好了，如果需要把当前对容器做出的修改同步到原有的镜像文件(推荐重新另存一个新的镜像文件)，可以使用<a href="https://kylin27.github.io/2016/02/13/2016.2.13.docker_install/">上一篇博客中</a>介绍的 docker commit 这个命令</p>
<h1 id="u5173_u4E8E_u6536_u5C3E_u5DE5_u4F5C"><a href="#u5173_u4E8E_u6536_u5C3E_u5DE5_u4F5C" class="headerlink" title="关于收尾工作"></a>关于收尾工作</h1><p>当前所处的状态是 scala&gt; 的命令行，输入 exit 便可以退出当前 scala 命令行交互的状态；<br>再次输入 exit (一次或是多次) 便可以退出当前登录的 spark-hadoop 集群容器，当然容器在你退出之后便会’消亡’,也就是不运行了系统回收它的资源咯，输入 docker ps 便查看不到容器信息；<br>如果在实际工作中推荐的做法是，在退出容器之前，在另一个远程访问终端内，将该容器的状态信息进行保存(归档或是生成镜像文件，如果乐意也可以将生成的镜像文件提交到 docker-hub 的上面)</p>
<p>总之，博客中很多地方写的很啰嗦啦，因为我喜欢在自己经常犯的错误的地方啰嗦几句，不喜欢的话，来打我啊~<br>end</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://kylin27.github.io/2016/02/14/2016.2.14.docker_spark/" data-id="cill04bc3002zrkimhkc1x3dn" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/cluster/">cluster</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/docker/">docker</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/dockerfile/">dockerfile</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/spark/">spark</a></li></ul>

    </footer>
  </div>
  
</article>


  
  
    <nav id="page-nav">
      <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="extend next" rel="next" href="/page/2/">Next &raquo;</a>
    </nav>
  
</section>
        
          <aside id="sidebar">
  
    
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tags</h3>
    <div class="widget">
      <ul class="tag-list"><li class="tag-list-item"><a class="tag-list-link" href="/tags/C/">C++</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Chord/">Chord</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/DHT/">DHT</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Gtest/">Gtest</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/IDEA/">IDEA</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Intellij/">Intellij</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Linux/">Linux</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Node-js/">Node.js</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/P2P/">P2P</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Spark/">Spark</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/StackExchangeRecommenderSystem-repo/">StackExchangeRecommenderSystem_repo</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Ubuntu/">Ubuntu</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/algorithm/">algorithm</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/blog/">blog</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/cluster/">cluster</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/concurrency/">concurrency</a><span class="tag-list-count">3</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/datastructure/">datastructure</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/debug/">debug</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/doc/">doc</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/docker/">docker</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/dockerfile/">dockerfile</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/github/">github</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/gtest/">gtest</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/hexo/">hexo</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/java/">java</a><span class="tag-list-count">5</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/log4j/">log4j</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/manual/">manual</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/note/">note</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/official/">official</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/official-guides/">official-guides</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/picture/">picture</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/programming-theory/">programming-theory</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/scala/">scala</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/search-engine/">search-engine</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/spark/">spark</a><span class="tag-list-count">3</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/theory/">theory</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/translate/">translate</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/translation/">translation</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/tutorial/">tutorial</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/windows/">windows</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/七牛/">七牛</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/个人简历/">个人简历</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/测试/">测试</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/翻译/">翻译</a><span class="tag-list-count">2</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/tags/C/" style="font-size: 10px;">C++</a> <a href="/tags/Chord/" style="font-size: 10px;">Chord</a> <a href="/tags/DHT/" style="font-size: 10px;">DHT</a> <a href="/tags/Gtest/" style="font-size: 10px;">Gtest</a> <a href="/tags/IDEA/" style="font-size: 10px;">IDEA</a> <a href="/tags/Intellij/" style="font-size: 10px;">Intellij</a> <a href="/tags/Linux/" style="font-size: 10px;">Linux</a> <a href="/tags/Node-js/" style="font-size: 10px;">Node.js</a> <a href="/tags/P2P/" style="font-size: 10px;">P2P</a> <a href="/tags/Spark/" style="font-size: 10px;">Spark</a> <a href="/tags/StackExchangeRecommenderSystem-repo/" style="font-size: 10px;">StackExchangeRecommenderSystem_repo</a> <a href="/tags/Ubuntu/" style="font-size: 10px;">Ubuntu</a> <a href="/tags/algorithm/" style="font-size: 10px;">algorithm</a> <a href="/tags/blog/" style="font-size: 13.33px;">blog</a> <a href="/tags/cluster/" style="font-size: 10px;">cluster</a> <a href="/tags/concurrency/" style="font-size: 16.67px;">concurrency</a> <a href="/tags/datastructure/" style="font-size: 10px;">datastructure</a> <a href="/tags/debug/" style="font-size: 10px;">debug</a> <a href="/tags/doc/" style="font-size: 10px;">doc</a> <a href="/tags/docker/" style="font-size: 13.33px;">docker</a> <a href="/tags/dockerfile/" style="font-size: 10px;">dockerfile</a> <a href="/tags/github/" style="font-size: 13.33px;">github</a> <a href="/tags/gtest/" style="font-size: 10px;">gtest</a> <a href="/tags/hexo/" style="font-size: 13.33px;">hexo</a> <a href="/tags/java/" style="font-size: 20px;">java</a> <a href="/tags/log4j/" style="font-size: 10px;">log4j</a> <a href="/tags/manual/" style="font-size: 10px;">manual</a> <a href="/tags/note/" style="font-size: 10px;">note</a> <a href="/tags/official/" style="font-size: 10px;">official</a> <a href="/tags/official-guides/" style="font-size: 10px;">official-guides</a> <a href="/tags/picture/" style="font-size: 10px;">picture</a> <a href="/tags/programming-theory/" style="font-size: 10px;">programming-theory</a> <a href="/tags/scala/" style="font-size: 13.33px;">scala</a> <a href="/tags/search-engine/" style="font-size: 10px;">search-engine</a> <a href="/tags/spark/" style="font-size: 16.67px;">spark</a> <a href="/tags/theory/" style="font-size: 13.33px;">theory</a> <a href="/tags/translate/" style="font-size: 10px;">translate</a> <a href="/tags/translation/" style="font-size: 13.33px;">translation</a> <a href="/tags/tutorial/" style="font-size: 10px;">tutorial</a> <a href="/tags/windows/" style="font-size: 10px;">windows</a> <a href="/tags/七牛/" style="font-size: 10px;">七牛</a> <a href="/tags/个人简历/" style="font-size: 10px;">个人简历</a> <a href="/tags/测试/" style="font-size: 10px;">测试</a> <a href="/tags/翻译/" style="font-size: 13.33px;">翻译</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/03/">March 2016</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/02/">February 2016</a><span class="archive-list-count">15</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/10/">October 2015</a><span class="archive-list-count">1</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recents</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2016/03/09/2016.3.8.cluster-overview1/">Spark 部署文档-Spark 集群模式概览-1</a>
          </li>
        
          <li>
            <a href="/2016/02/27/2016.2.27.java_multithreading_costs3/">Multithreading Costs [多线程所花费的代价]</a>
          </li>
        
          <li>
            <a href="/2016/02/27/2016.27.java_concurrency.Multithreading_benifits2/">Multithreading Benefits [多线程带来的福利]</a>
          </li>
        
          <li>
            <a href="/2016/02/27/2016.2.27.java_concurrency.Multithreading Tutorial_1/">Java Concurrency / Multithreading Tutorial [Java 并发/多线程教程]</a>
          </li>
        
          <li>
            <a href="/2016/02/26/2016.2.26.remote_debug_spark_intellij/">基于单节点的 Spark &amp; IDEA 远程调试</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2016 Kylin<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css" type="text/css">
  <script src="/fancybox/jquery.fancybox.pack.js" type="text/javascript"></script>


<script src="/js/script.js" type="text/javascript"></script>

  </div>
</body>
</html>